{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XZkJpkYK8eXD",
    "outputId": "5ef7d4a5-d01d-44ab-886f-9e4f1f78329a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "JNcgeT9x8fk7",
    "outputId": "4fa75c3a-69e7-4bf8-b6f4-5f485cbacd91",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting patool\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
      "\r",
      "\u001b[K     |████▎                           | 10kB 13.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 20kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 30kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 40kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 51kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 61kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 71kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 81kB 2.5MB/s \n",
      "\u001b[?25hInstalling collected packages: patool\n",
      "Successfully installed patool-1.12\n",
      "patool: Extracting /content/drive/My Drive/Autopilot-TensorFlow-master.rar ...\n",
      "patool: running /usr/bin/unrar x -- \"/content/drive/My Drive/Autopilot-TensorFlow-master.rar\"\n",
      "patool:     with cwd='./Unpack_gce_1zzn'\n",
      "patool: ... /content/drive/My Drive/Autopilot-TensorFlow-master.rar extracted to `Autopilot-TensorFlow-master'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Autopilot-TensorFlow-master'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install patool\n",
    "import patoolib\n",
    "patoolib.extract_archive('/content/drive/My Drive/Autopilot-TensorFlow-master.rar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lading of the Driving_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUJZUaAuglrl"
   },
   "outputs": [],
   "source": [
    "#  !pip install scipy \n",
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "#points to the end of the last batch\n",
    "train_batch_pointer = 0\n",
    "val_batch_pointer = 0\n",
    "\n",
    "#read data.txt\n",
    "with open(\"/content/Autopilot-TensorFlow-master/Autopilot-TensorFlow-master/driving_dataset/data.txt\") as f:\n",
    "    for line in f:\n",
    "        xs.append(\"/content/Autopilot-TensorFlow-master/Autopilot-TensorFlow-master/driving_dataset/\" + line.split()[0])\n",
    "        #the paper by Nvidia uses the inverse of the turning radius,\n",
    "        #but steering wheel angle is proportional to the inverse of turning radius\n",
    "        #so the steering wheel angle in radians is used as the output\n",
    "        ys.append(float(line.split()[1]) * scipy.pi / 180)\n",
    "\n",
    "#get number of images\n",
    "num_images = len(xs)\n",
    "\n",
    "import imageio\n",
    "import skimage.transform \n",
    "\n",
    "train_xs = xs[:int(len(xs) * 0.7)]\n",
    "train_ys = ys[:int(len(xs) * 0.7)]\n",
    "\n",
    "val_xs = xs[-int(len(xs) * 0.3):]\n",
    "val_ys = ys[-int(len(xs) * 0.3):]\n",
    "\n",
    "num_train_images = len(train_xs)\n",
    "num_val_images = len(val_xs)\n",
    "\n",
    "def LoadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(skimage.transform.resize(imageio.imread(train_xs[(train_batch_pointer + i) % num_train_images])[-150:],[66, 200]) / 255.0)\n",
    "        y_out.append([train_ys[(train_batch_pointer + i) % num_train_images]])\n",
    "    train_batch_pointer += batch_size\n",
    "    return x_out, y_out\n",
    "\n",
    "def LoadValBatch(batch_size):\n",
    "    global val_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(skimage.transform.resize(imageio.imread(val_xs[(val_batch_pointer + i) % num_val_images])[-150:], [66, 200]) / 255.0)\n",
    "        y_out.append([val_ys[(val_batch_pointer + i) % num_val_images]])\n",
    "    val_batch_pointer += batch_size\n",
    "    return x_out, y_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN_CNN---Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "Ls-qP224l4qT",
    "outputId": "2f9993f0-27a2-46ab-e48a-bc9fa9ade0a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-8a4f20c9206a>:58: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')\n",
    "\n",
    "x = tf.compat.v1.placeholder(tf.float32, shape=[None, 66, 200, 3])\n",
    "y_ = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "x_image = x\n",
    "\n",
    "#first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 24])\n",
    "b_conv1 = bias_variable([24])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 2) + b_conv1)\n",
    "\n",
    "#second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 24, 36])\n",
    "b_conv2 = bias_variable([36])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2) + b_conv2)\n",
    "\n",
    "#third convolutional layer\n",
    "W_conv3 = weight_variable([5, 5, 36, 48])\n",
    "b_conv3 = bias_variable([48])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 2) + b_conv3)\n",
    "\n",
    "#fourth convolutional layer\n",
    "W_conv4 = weight_variable([3, 3, 48, 64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4, 1) + b_conv4)\n",
    "\n",
    "#fifth convolutional layer\n",
    "W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "b_conv5 = bias_variable([64])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, 1) + b_conv5)\n",
    "\n",
    "#FCL 1\n",
    "W_fc1 = weight_variable([1152, 1164])\n",
    "b_fc1 = bias_variable([1164])\n",
    "\n",
    "h_conv5_flat = tf.reshape(h_conv5, [-1, 1152])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#FCL 2\n",
    "W_fc2 = weight_variable([1164, 100])\n",
    "b_fc2 = bias_variable([100])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc3 = weight_variable([100, 50])\n",
    "b_fc3 = bias_variable([50])\n",
    "\n",
    "h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc4 = weight_variable([50, 10])\n",
    "b_fc4 = bias_variable([10])\n",
    "\n",
    "h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop, W_fc4) + b_fc4)\n",
    "\n",
    "h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)\n",
    "\n",
    "#Output\n",
    "W_fc5 = weight_variable([10, 1])\n",
    "b_fc5 = bias_variable([1])\n",
    "\n",
    "y = tf.multiply(tf.matmul(h_fc4_drop, W_fc5) + b_fc5, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluvation of Loss using Adam optimizer(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UJpoOtsh0a_s",
    "outputId": "3a34199b-eabc-4102-874c-057b7ebcea0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch: 0, Step: 0, Loss: 6.62516\n",
      "Epoch: 0, Step: 10, Loss: 5.54357\n",
      "Epoch: 0, Step: 20, Loss: 5.04342\n",
      "Epoch: 0, Step: 30, Loss: 4.64451\n",
      "Epoch: 0, Step: 40, Loss: 4.4996\n",
      "Epoch: 0, Step: 50, Loss: 3.98863\n",
      "Epoch: 0, Step: 60, Loss: 3.94773\n",
      "Epoch: 0, Step: 70, Loss: 3.96057\n",
      "Epoch: 0, Step: 80, Loss: 3.6136\n",
      "Epoch: 0, Step: 90, Loss: 3.17738\n",
      "Epoch: 0, Step: 100, Loss: 3.01685\n",
      "Epoch: 0, Step: 110, Loss: 2.80845\n",
      "Epoch: 0, Step: 120, Loss: 2.69694\n",
      "Epoch: 0, Step: 130, Loss: 2.95446\n",
      "Epoch: 0, Step: 140, Loss: 3.39662\n",
      "Epoch: 0, Step: 150, Loss: 2.63709\n",
      "Epoch: 0, Step: 160, Loss: 3.15956\n",
      "Epoch: 0, Step: 170, Loss: 2.46703\n",
      "Epoch: 0, Step: 180, Loss: 2.80248\n",
      "Epoch: 0, Step: 190, Loss: 2.45786\n",
      "Epoch: 0, Step: 200, Loss: 2.0313\n",
      "Epoch: 0, Step: 210, Loss: 1.95679\n",
      "Epoch: 0, Step: 220, Loss: 1.937\n",
      "Epoch: 0, Step: 230, Loss: 1.97091\n",
      "Epoch: 0, Step: 240, Loss: 2.84514\n",
      "Epoch: 0, Step: 250, Loss: 1.93149\n",
      "Epoch: 0, Step: 260, Loss: 3.99353\n",
      "Epoch: 0, Step: 270, Loss: 2.2059\n",
      "Epoch: 0, Step: 280, Loss: 2.72868\n",
      "Epoch: 0, Step: 290, Loss: 1.66485\n",
      "Epoch: 0, Step: 300, Loss: 1.53813\n",
      "Epoch: 0, Step: 310, Loss: 1.71267\n",
      "Epoch: 0, Step: 320, Loss: 2.78807\n",
      "Epoch: 0, Step: 330, Loss: 1.95668\n",
      "Epoch: 0, Step: 340, Loss: 1.58192\n",
      "Epoch: 0, Step: 350, Loss: 1.39967\n",
      "Epoch: 0, Step: 360, Loss: 1.40188\n",
      "Epoch: 0, Step: 370, Loss: 1.59278\n",
      "Epoch: 0, Step: 380, Loss: 1.59224\n",
      "Epoch: 0, Step: 390, Loss: 2.69611\n",
      "Epoch: 0, Step: 400, Loss: 2.05397\n",
      "Epoch: 0, Step: 410, Loss: 1.23631\n",
      "Epoch: 0, Step: 420, Loss: 1.20005\n",
      "Epoch: 0, Step: 430, Loss: 1.18221\n",
      "Epoch: 0, Step: 440, Loss: 1.1611\n",
      "Epoch: 0, Step: 450, Loss: 1.17356\n",
      "Epoch: 1, Step: 100, Loss: 1.5772\n",
      "Epoch: 1, Step: 110, Loss: 1.48284\n",
      "Epoch: 1, Step: 120, Loss: 1.15119\n",
      "Epoch: 1, Step: 130, Loss: 1.05217\n",
      "Epoch: 1, Step: 140, Loss: 1.01885\n",
      "Epoch: 1, Step: 150, Loss: 1.04592\n",
      "Epoch: 1, Step: 160, Loss: 1.38758\n",
      "Epoch: 1, Step: 170, Loss: 2.43834\n",
      "Epoch: 1, Step: 180, Loss: 1.64444\n",
      "Epoch: 1, Step: 190, Loss: 0.990733\n",
      "Epoch: 1, Step: 200, Loss: 0.91748\n",
      "Epoch: 1, Step: 210, Loss: 0.878566\n",
      "Epoch: 1, Step: 220, Loss: 0.857635\n",
      "Epoch: 1, Step: 230, Loss: 0.849112\n",
      "Epoch: 1, Step: 240, Loss: 0.855894\n",
      "Epoch: 1, Step: 250, Loss: 0.875273\n",
      "Epoch: 1, Step: 260, Loss: 0.796976\n",
      "Epoch: 1, Step: 270, Loss: 0.838621\n",
      "Epoch: 1, Step: 280, Loss: 0.89107\n",
      "Epoch: 1, Step: 290, Loss: 1.12306\n",
      "Epoch: 1, Step: 300, Loss: 0.921112\n",
      "Epoch: 1, Step: 310, Loss: 0.73297\n",
      "Epoch: 1, Step: 320, Loss: 0.71799\n",
      "Epoch: 1, Step: 330, Loss: 0.703777\n",
      "Epoch: 1, Step: 340, Loss: 0.691285\n",
      "Epoch: 1, Step: 350, Loss: 0.682759\n",
      "Epoch: 1, Step: 360, Loss: 2.49333\n",
      "Epoch: 1, Step: 370, Loss: 4.62057\n",
      "Epoch: 1, Step: 380, Loss: 0.656324\n",
      "Epoch: 1, Step: 390, Loss: 0.636661\n",
      "Epoch: 1, Step: 400, Loss: 0.627228\n",
      "Epoch: 1, Step: 410, Loss: 0.621881\n",
      "Epoch: 1, Step: 420, Loss: 0.606437\n",
      "Epoch: 1, Step: 430, Loss: 0.597974\n",
      "Epoch: 1, Step: 440, Loss: 0.586953\n",
      "Epoch: 1, Step: 450, Loss: 0.576411\n",
      "Epoch: 1, Step: 460, Loss: 0.567376\n",
      "Epoch: 1, Step: 470, Loss: 0.556299\n",
      "Epoch: 1, Step: 480, Loss: 0.553207\n",
      "Epoch: 1, Step: 490, Loss: 0.578976\n",
      "Epoch: 1, Step: 500, Loss: 0.568398\n",
      "Epoch: 1, Step: 510, Loss: 0.52892\n",
      "Epoch: 1, Step: 520, Loss: 0.514068\n",
      "Epoch: 1, Step: 530, Loss: 0.506727\n",
      "Epoch: 1, Step: 540, Loss: 0.499738\n",
      "Epoch: 1, Step: 550, Loss: 0.491495\n",
      "Epoch: 2, Step: 200, Loss: 0.489225\n",
      "Epoch: 2, Step: 210, Loss: 0.482268\n",
      "Epoch: 2, Step: 220, Loss: 0.474177\n",
      "Epoch: 2, Step: 230, Loss: 0.468216\n",
      "Epoch: 2, Step: 240, Loss: 0.460821\n",
      "Epoch: 2, Step: 250, Loss: 0.454462\n",
      "Epoch: 2, Step: 260, Loss: 0.449252\n",
      "Epoch: 2, Step: 270, Loss: 0.444895\n",
      "Epoch: 2, Step: 280, Loss: 0.438323\n",
      "Epoch: 2, Step: 290, Loss: 0.43329\n",
      "Epoch: 2, Step: 300, Loss: 0.473738\n",
      "Epoch: 2, Step: 310, Loss: 0.445972\n",
      "Epoch: 2, Step: 320, Loss: 4.53554\n",
      "Epoch: 2, Step: 330, Loss: 2.01785\n",
      "Epoch: 2, Step: 340, Loss: 0.406086\n",
      "Epoch: 2, Step: 350, Loss: 0.404003\n",
      "Epoch: 2, Step: 360, Loss: 0.435352\n",
      "Epoch: 2, Step: 370, Loss: 0.449971\n",
      "Epoch: 2, Step: 380, Loss: 0.393976\n",
      "Epoch: 2, Step: 390, Loss: 0.462624\n",
      "Epoch: 2, Step: 400, Loss: 0.467709\n",
      "Epoch: 2, Step: 410, Loss: 0.364623\n",
      "Epoch: 2, Step: 420, Loss: 0.39773\n",
      "Epoch: 2, Step: 430, Loss: 0.369421\n",
      "Epoch: 2, Step: 440, Loss: 0.362611\n",
      "Epoch: 2, Step: 450, Loss: 0.347698\n",
      "Epoch: 2, Step: 460, Loss: 0.475283\n",
      "Epoch: 2, Step: 470, Loss: 0.389703\n",
      "Epoch: 2, Step: 480, Loss: 0.448855\n",
      "Epoch: 2, Step: 490, Loss: 0.41743\n",
      "Epoch: 2, Step: 500, Loss: 0.352208\n",
      "Epoch: 2, Step: 510, Loss: 0.344603\n",
      "Epoch: 2, Step: 520, Loss: 0.344974\n",
      "Epoch: 2, Step: 530, Loss: 0.335386\n",
      "Epoch: 2, Step: 540, Loss: 0.330606\n",
      "Epoch: 2, Step: 550, Loss: 0.326669\n",
      "Epoch: 2, Step: 560, Loss: 0.322351\n",
      "Epoch: 2, Step: 570, Loss: 0.315528\n",
      "Epoch: 2, Step: 580, Loss: 0.315964\n",
      "Epoch: 2, Step: 590, Loss: 0.306695\n",
      "Epoch: 2, Step: 600, Loss: 0.298547\n",
      "Epoch: 2, Step: 610, Loss: 0.290032\n",
      "Epoch: 2, Step: 620, Loss: 0.319242\n",
      "Epoch: 2, Step: 630, Loss: 0.3192\n",
      "Epoch: 2, Step: 640, Loss: 0.626038\n",
      "Epoch: 2, Step: 650, Loss: 0.286074\n",
      "Epoch: 3, Step: 300, Loss: 0.316861\n",
      "Epoch: 3, Step: 310, Loss: 0.33637\n",
      "Epoch: 3, Step: 320, Loss: 0.607331\n",
      "Epoch: 3, Step: 330, Loss: 0.341232\n",
      "Epoch: 3, Step: 340, Loss: 0.337463\n",
      "Epoch: 3, Step: 350, Loss: 0.560302\n",
      "Epoch: 3, Step: 360, Loss: 0.535337\n",
      "Epoch: 3, Step: 370, Loss: 0.25699\n",
      "Epoch: 3, Step: 380, Loss: 0.281489\n",
      "Epoch: 3, Step: 390, Loss: 0.246431\n",
      "Epoch: 3, Step: 400, Loss: 0.284549\n",
      "Epoch: 3, Step: 410, Loss: 0.310045\n",
      "Epoch: 3, Step: 420, Loss: 1.09179\n",
      "Epoch: 3, Step: 430, Loss: 0.406847\n",
      "Epoch: 3, Step: 440, Loss: 1.16503\n",
      "Epoch: 3, Step: 450, Loss: 0.780464\n",
      "Epoch: 3, Step: 460, Loss: 0.634954\n",
      "Epoch: 3, Step: 470, Loss: 0.611844\n",
      "Epoch: 3, Step: 480, Loss: 0.233122\n",
      "Epoch: 3, Step: 490, Loss: 0.23591\n",
      "Epoch: 3, Step: 500, Loss: 0.259404\n",
      "Epoch: 3, Step: 510, Loss: 0.400939\n",
      "Epoch: 3, Step: 520, Loss: 0.966629\n",
      "Epoch: 3, Step: 530, Loss: 0.501282\n",
      "Epoch: 3, Step: 540, Loss: 2.10386\n",
      "Epoch: 3, Step: 550, Loss: 0.886246\n",
      "Epoch: 3, Step: 560, Loss: 1.34226\n",
      "Epoch: 3, Step: 570, Loss: 0.418069\n",
      "Epoch: 3, Step: 580, Loss: 0.195822\n",
      "Epoch: 3, Step: 590, Loss: 0.284168\n",
      "Epoch: 3, Step: 600, Loss: 1.32968\n",
      "Epoch: 3, Step: 610, Loss: 0.981513\n",
      "Epoch: 3, Step: 620, Loss: 0.295589\n",
      "Epoch: 3, Step: 630, Loss: 0.244325\n",
      "Epoch: 3, Step: 640, Loss: 0.224285\n",
      "Epoch: 3, Step: 650, Loss: 0.573067\n",
      "Epoch: 3, Step: 660, Loss: 0.345416\n",
      "Epoch: 3, Step: 670, Loss: 1.23102\n",
      "Epoch: 3, Step: 680, Loss: 1.05605\n",
      "Epoch: 3, Step: 690, Loss: 0.313906\n",
      "Epoch: 3, Step: 700, Loss: 0.174438\n",
      "Epoch: 3, Step: 710, Loss: 0.181881\n",
      "Epoch: 3, Step: 720, Loss: 0.189187\n",
      "Epoch: 3, Step: 730, Loss: 0.198012\n",
      "Epoch: 3, Step: 740, Loss: 0.66765\n",
      "Epoch: 3, Step: 750, Loss: 0.741554\n",
      "Epoch: 4, Step: 400, Loss: 0.337856\n",
      "Epoch: 4, Step: 410, Loss: 0.194795\n",
      "Epoch: 4, Step: 420, Loss: 0.182013\n",
      "Epoch: 4, Step: 430, Loss: 0.174534\n",
      "Epoch: 4, Step: 440, Loss: 0.3409\n",
      "Epoch: 4, Step: 450, Loss: 1.41535\n",
      "Epoch: 4, Step: 460, Loss: 1.12211\n",
      "Epoch: 4, Step: 470, Loss: 0.20449\n",
      "Epoch: 4, Step: 480, Loss: 0.229657\n",
      "Epoch: 4, Step: 490, Loss: 0.144928\n",
      "Epoch: 4, Step: 500, Loss: 0.143578\n",
      "Epoch: 4, Step: 510, Loss: 0.147985\n",
      "Epoch: 4, Step: 520, Loss: 0.167962\n",
      "Epoch: 4, Step: 530, Loss: 0.207968\n",
      "Epoch: 4, Step: 540, Loss: 0.14403\n",
      "Epoch: 4, Step: 550, Loss: 0.155613\n",
      "Epoch: 4, Step: 560, Loss: 0.249251\n",
      "Epoch: 4, Step: 570, Loss: 0.499985\n",
      "Epoch: 4, Step: 580, Loss: 0.324461\n",
      "Epoch: 4, Step: 590, Loss: 0.137876\n",
      "Epoch: 4, Step: 600, Loss: 0.13197\n",
      "Epoch: 4, Step: 610, Loss: 0.13226\n",
      "Epoch: 4, Step: 620, Loss: 0.131327\n",
      "Epoch: 4, Step: 630, Loss: 0.129237\n",
      "Epoch: 4, Step: 640, Loss: 0.65174\n",
      "Epoch: 4, Step: 650, Loss: 5.05979\n",
      "Epoch: 4, Step: 660, Loss: 0.161224\n",
      "Epoch: 4, Step: 670, Loss: 0.127162\n",
      "Epoch: 4, Step: 680, Loss: 0.122375\n",
      "Epoch: 4, Step: 690, Loss: 0.126751\n",
      "Epoch: 4, Step: 700, Loss: 0.122128\n",
      "Epoch: 4, Step: 710, Loss: 0.118933\n",
      "Epoch: 4, Step: 720, Loss: 0.113889\n",
      "Epoch: 4, Step: 730, Loss: 0.112392\n",
      "Epoch: 4, Step: 740, Loss: 0.111206\n",
      "Epoch: 4, Step: 750, Loss: 0.109045\n",
      "Epoch: 4, Step: 760, Loss: 0.110588\n",
      "Epoch: 4, Step: 770, Loss: 0.14713\n",
      "Epoch: 4, Step: 780, Loss: 0.151441\n",
      "Epoch: 4, Step: 790, Loss: 0.120803\n",
      "Epoch: 4, Step: 800, Loss: 0.103027\n",
      "Epoch: 4, Step: 810, Loss: 0.103007\n",
      "Epoch: 4, Step: 820, Loss: 0.101864\n",
      "Epoch: 4, Step: 830, Loss: 0.101009\n",
      "Epoch: 4, Step: 840, Loss: 0.100158\n",
      "Epoch: 4, Step: 850, Loss: 0.100091\n",
      "Epoch: 5, Step: 500, Loss: 0.0986492\n",
      "Epoch: 5, Step: 510, Loss: 0.100785\n",
      "Epoch: 5, Step: 520, Loss: 0.0986179\n",
      "Epoch: 5, Step: 530, Loss: 0.0985106\n",
      "Epoch: 5, Step: 540, Loss: 0.0976641\n",
      "Epoch: 5, Step: 550, Loss: 0.0970079\n",
      "Epoch: 5, Step: 560, Loss: 0.0943989\n",
      "Epoch: 5, Step: 570, Loss: 0.0940851\n",
      "Epoch: 5, Step: 580, Loss: 0.102512\n",
      "Epoch: 5, Step: 590, Loss: 0.147376\n",
      "Epoch: 5, Step: 600, Loss: 2.87272\n",
      "Epoch: 5, Step: 610, Loss: 2.85135\n",
      "Epoch: 5, Step: 620, Loss: 0.0893359\n",
      "Epoch: 5, Step: 630, Loss: 0.0866827\n",
      "Epoch: 5, Step: 640, Loss: 0.145361\n",
      "Epoch: 5, Step: 650, Loss: 0.151251\n",
      "Epoch: 5, Step: 660, Loss: 0.134233\n",
      "Epoch: 5, Step: 670, Loss: 0.133293\n",
      "Epoch: 5, Step: 680, Loss: 0.192336\n",
      "Epoch: 5, Step: 690, Loss: 0.0838949\n",
      "Epoch: 5, Step: 700, Loss: 0.106976\n",
      "Epoch: 5, Step: 710, Loss: 0.101333\n",
      "Epoch: 5, Step: 720, Loss: 0.0952201\n",
      "Epoch: 5, Step: 730, Loss: 0.0851577\n",
      "Epoch: 5, Step: 740, Loss: 0.184188\n",
      "Epoch: 5, Step: 750, Loss: 0.174494\n",
      "Epoch: 5, Step: 760, Loss: 0.157254\n",
      "Epoch: 5, Step: 770, Loss: 0.186\n",
      "Epoch: 5, Step: 780, Loss: 0.0932534\n",
      "Epoch: 5, Step: 790, Loss: 0.0992035\n",
      "Epoch: 5, Step: 800, Loss: 0.0885102\n",
      "Epoch: 5, Step: 810, Loss: 0.0853456\n",
      "Epoch: 5, Step: 820, Loss: 0.0829075\n",
      "Epoch: 5, Step: 830, Loss: 0.0807749\n",
      "Epoch: 5, Step: 840, Loss: 0.0810359\n",
      "Epoch: 5, Step: 850, Loss: 0.0788651\n",
      "Epoch: 5, Step: 860, Loss: 0.0892546\n",
      "Epoch: 5, Step: 870, Loss: 0.084113\n",
      "Epoch: 5, Step: 880, Loss: 0.0836123\n",
      "Epoch: 5, Step: 890, Loss: 0.072056\n",
      "Epoch: 5, Step: 900, Loss: 0.0999498\n",
      "Epoch: 5, Step: 910, Loss: 0.136234\n",
      "Epoch: 5, Step: 920, Loss: 0.352769\n",
      "Epoch: 5, Step: 930, Loss: 0.107052\n",
      "Epoch: 5, Step: 940, Loss: 0.110196\n",
      "Epoch: 5, Step: 950, Loss: 0.176267\n",
      "Epoch: 6, Step: 600, Loss: 0.362979\n",
      "Epoch: 6, Step: 610, Loss: 0.259309\n",
      "Epoch: 6, Step: 620, Loss: 0.0891433\n",
      "Epoch: 6, Step: 630, Loss: 0.314936\n",
      "Epoch: 6, Step: 640, Loss: 0.37232\n",
      "Epoch: 6, Step: 650, Loss: 0.0771962\n",
      "Epoch: 6, Step: 660, Loss: 0.102326\n",
      "Epoch: 6, Step: 670, Loss: 0.0632094\n",
      "Epoch: 6, Step: 680, Loss: 0.109513\n",
      "Epoch: 6, Step: 690, Loss: 0.0810216\n",
      "Epoch: 6, Step: 700, Loss: 0.846738\n",
      "Epoch: 6, Step: 710, Loss: 0.247802\n",
      "Epoch: 6, Step: 720, Loss: 0.920461\n",
      "Epoch: 6, Step: 730, Loss: 0.82733\n",
      "Epoch: 6, Step: 740, Loss: 0.314992\n",
      "Epoch: 6, Step: 750, Loss: 0.582075\n",
      "Epoch: 6, Step: 760, Loss: 0.120055\n",
      "Epoch: 6, Step: 770, Loss: 0.065804\n",
      "Epoch: 6, Step: 780, Loss: 0.0791868\n",
      "Epoch: 6, Step: 790, Loss: 0.21822\n",
      "Epoch: 6, Step: 800, Loss: 0.522903\n",
      "Epoch: 6, Step: 810, Loss: 0.747209\n",
      "Epoch: 6, Step: 820, Loss: 1.33583\n",
      "Epoch: 6, Step: 830, Loss: 1.20799\n",
      "Epoch: 6, Step: 840, Loss: 1.06715\n",
      "Epoch: 6, Step: 850, Loss: 0.479258\n",
      "Epoch: 6, Step: 860, Loss: 0.0501721\n",
      "Epoch: 6, Step: 870, Loss: 0.0842704\n",
      "Epoch: 6, Step: 880, Loss: 0.973464\n",
      "Epoch: 6, Step: 890, Loss: 1.22722\n",
      "Epoch: 6, Step: 900, Loss: 0.108789\n",
      "Epoch: 6, Step: 910, Loss: 0.128187\n",
      "Epoch: 6, Step: 920, Loss: 0.058664\n",
      "Epoch: 6, Step: 930, Loss: 0.362589\n",
      "Epoch: 6, Step: 940, Loss: 0.256398\n",
      "Epoch: 6, Step: 950, Loss: 0.832826\n",
      "Epoch: 6, Step: 960, Loss: 1.08944\n",
      "Epoch: 6, Step: 970, Loss: 0.299345\n",
      "Epoch: 6, Step: 980, Loss: 0.0475722\n",
      "Epoch: 6, Step: 990, Loss: 0.0569356\n",
      "Epoch: 6, Step: 1000, Loss: 0.065597\n",
      "Epoch: 6, Step: 1010, Loss: 0.0713799\n",
      "Epoch: 6, Step: 1020, Loss: 0.447329\n",
      "Epoch: 6, Step: 1030, Loss: 0.6828\n",
      "Epoch: 6, Step: 1040, Loss: 0.284197\n",
      "Epoch: 6, Step: 1050, Loss: 0.106594\n",
      "Epoch: 7, Step: 700, Loss: 0.0865673\n",
      "Epoch: 7, Step: 710, Loss: 0.0524354\n",
      "Epoch: 7, Step: 720, Loss: 0.170344\n",
      "Epoch: 7, Step: 730, Loss: 1.10449\n",
      "Epoch: 7, Step: 740, Loss: 1.40213\n",
      "Epoch: 7, Step: 750, Loss: 0.117123\n",
      "Epoch: 7, Step: 760, Loss: 0.138788\n",
      "Epoch: 7, Step: 770, Loss: 0.0384932\n",
      "Epoch: 7, Step: 780, Loss: 0.0472037\n",
      "Epoch: 7, Step: 790, Loss: 0.0450955\n",
      "Epoch: 7, Step: 800, Loss: 0.0600006\n",
      "Epoch: 7, Step: 810, Loss: 0.0927722\n",
      "Epoch: 7, Step: 820, Loss: 0.0692744\n",
      "Epoch: 7, Step: 830, Loss: 0.0428723\n",
      "Epoch: 7, Step: 840, Loss: 0.145708\n",
      "Epoch: 7, Step: 850, Loss: 0.445628\n",
      "Epoch: 7, Step: 860, Loss: 0.162644\n",
      "Epoch: 7, Step: 870, Loss: 0.143454\n",
      "Epoch: 7, Step: 880, Loss: 0.0451939\n",
      "Epoch: 7, Step: 890, Loss: 0.0447438\n",
      "Epoch: 7, Step: 900, Loss: 0.0444793\n",
      "Epoch: 7, Step: 910, Loss: 0.0438745\n",
      "Epoch: 7, Step: 920, Loss: 0.0928449\n",
      "Epoch: 7, Step: 930, Loss: 5.23304\n",
      "Epoch: 7, Step: 940, Loss: 0.218535\n",
      "Epoch: 7, Step: 950, Loss: 0.0434056\n",
      "Epoch: 7, Step: 960, Loss: 0.0370879\n",
      "Epoch: 7, Step: 970, Loss: 0.0440562\n",
      "Epoch: 7, Step: 980, Loss: 0.04409\n",
      "Epoch: 7, Step: 990, Loss: 0.0384544\n",
      "Epoch: 7, Step: 1000, Loss: 0.0309935\n",
      "Epoch: 7, Step: 1010, Loss: 0.0308265\n",
      "Epoch: 7, Step: 1020, Loss: 0.0300813\n",
      "Epoch: 7, Step: 1030, Loss: 0.0307559\n",
      "Epoch: 7, Step: 1040, Loss: 0.0331182\n",
      "Epoch: 7, Step: 1050, Loss: 0.0477538\n",
      "Epoch: 7, Step: 1060, Loss: 0.0655509\n",
      "Epoch: 7, Step: 1070, Loss: 0.0454397\n",
      "Epoch: 7, Step: 1080, Loss: 0.0306556\n",
      "Epoch: 7, Step: 1090, Loss: 0.0282129\n",
      "Epoch: 7, Step: 1100, Loss: 0.0305391\n",
      "Epoch: 7, Step: 1110, Loss: 0.0290887\n",
      "Epoch: 7, Step: 1120, Loss: 0.0275436\n",
      "Epoch: 7, Step: 1130, Loss: 0.0300068\n",
      "Epoch: 7, Step: 1140, Loss: 0.0263308\n",
      "Epoch: 7, Step: 1150, Loss: 0.0274305\n",
      "Epoch: 8, Step: 800, Loss: 0.0281711\n",
      "Epoch: 8, Step: 810, Loss: 0.0274522\n",
      "Epoch: 8, Step: 820, Loss: 0.0262143\n",
      "Epoch: 8, Step: 830, Loss: 0.0256711\n",
      "Epoch: 8, Step: 840, Loss: 0.0247927\n",
      "Epoch: 8, Step: 850, Loss: 0.0249881\n",
      "Epoch: 8, Step: 860, Loss: 0.0284874\n",
      "Epoch: 8, Step: 870, Loss: 0.0846298\n",
      "Epoch: 8, Step: 880, Loss: 1.46034\n",
      "Epoch: 8, Step: 890, Loss: 4.03501\n",
      "Epoch: 8, Step: 900, Loss: 0.0297692\n",
      "Epoch: 8, Step: 910, Loss: 0.02351\n",
      "Epoch: 8, Step: 920, Loss: 0.0746438\n",
      "Epoch: 8, Step: 930, Loss: 0.0928522\n",
      "Epoch: 8, Step: 940, Loss: 0.0953119\n",
      "Epoch: 8, Step: 950, Loss: 0.0431098\n",
      "Epoch: 8, Step: 960, Loss: 0.140498\n",
      "Epoch: 8, Step: 970, Loss: 0.035143\n",
      "Epoch: 8, Step: 980, Loss: 0.031707\n",
      "Epoch: 8, Step: 990, Loss: 0.0570934\n",
      "Epoch: 8, Step: 1000, Loss: 0.0334069\n",
      "Epoch: 8, Step: 1010, Loss: 0.0354172\n",
      "Epoch: 8, Step: 1020, Loss: 0.0852969\n",
      "Epoch: 8, Step: 1030, Loss: 0.126048\n",
      "Epoch: 8, Step: 1040, Loss: 0.101867\n",
      "Epoch: 8, Step: 1050, Loss: 0.161869\n",
      "Epoch: 8, Step: 1060, Loss: 0.0483167\n",
      "Epoch: 8, Step: 1070, Loss: 0.0454199\n",
      "Epoch: 8, Step: 1080, Loss: 0.0326122\n",
      "Epoch: 8, Step: 1090, Loss: 0.0304008\n",
      "Epoch: 8, Step: 1100, Loss: 0.0322651\n",
      "Epoch: 8, Step: 1110, Loss: 0.0326497\n",
      "Epoch: 8, Step: 1120, Loss: 0.0319372\n",
      "Epoch: 8, Step: 1130, Loss: 0.0324195\n",
      "Epoch: 8, Step: 1140, Loss: 0.0466098\n",
      "Epoch: 8, Step: 1150, Loss: 0.0405673\n",
      "Epoch: 8, Step: 1160, Loss: 0.0483563\n",
      "Epoch: 8, Step: 1170, Loss: 0.0205786\n",
      "Epoch: 8, Step: 1180, Loss: 0.0493816\n",
      "Epoch: 8, Step: 1190, Loss: 0.112603\n",
      "Epoch: 8, Step: 1200, Loss: 0.204732\n",
      "Epoch: 8, Step: 1210, Loss: 0.142661\n",
      "Epoch: 8, Step: 1220, Loss: 0.0487628\n",
      "Epoch: 8, Step: 1230, Loss: 0.133325\n",
      "Epoch: 8, Step: 1240, Loss: 0.225535\n",
      "Epoch: 8, Step: 1250, Loss: 0.335155\n",
      "Epoch: 9, Step: 900, Loss: 0.0279556\n",
      "Epoch: 9, Step: 910, Loss: 0.225602\n",
      "Epoch: 9, Step: 920, Loss: 0.351527\n",
      "Epoch: 9, Step: 930, Loss: 0.0451138\n",
      "Epoch: 9, Step: 940, Loss: 0.0606595\n",
      "Epoch: 9, Step: 950, Loss: 0.0179724\n",
      "Epoch: 9, Step: 960, Loss: 0.0564644\n",
      "Epoch: 9, Step: 970, Loss: 0.0355805\n",
      "Epoch: 9, Step: 980, Loss: 0.749984\n",
      "Epoch: 9, Step: 990, Loss: 0.304044\n",
      "Epoch: 9, Step: 1000, Loss: 0.687214\n",
      "Epoch: 9, Step: 1010, Loss: 0.850859\n",
      "Epoch: 9, Step: 1020, Loss: 0.164757\n",
      "Epoch: 9, Step: 1030, Loss: 0.670774\n",
      "Epoch: 9, Step: 1040, Loss: 0.161245\n",
      "Epoch: 9, Step: 1050, Loss: 0.0207865\n",
      "Epoch: 9, Step: 1060, Loss: 0.0358046\n",
      "Epoch: 9, Step: 1070, Loss: 0.162669\n",
      "Epoch: 9, Step: 1080, Loss: 0.256654\n",
      "Epoch: 9, Step: 1090, Loss: 0.930414\n",
      "Epoch: 9, Step: 1100, Loss: 0.780248\n",
      "Epoch: 9, Step: 1110, Loss: 1.79732\n",
      "Epoch: 9, Step: 1120, Loss: 0.733178\n",
      "Epoch: 9, Step: 1130, Loss: 0.649865\n",
      "Epoch: 9, Step: 1140, Loss: 0.0147145\n",
      "Epoch: 9, Step: 1150, Loss: 0.0265051\n",
      "Epoch: 9, Step: 1160, Loss: 0.732781\n",
      "Epoch: 9, Step: 1170, Loss: 1.46493\n",
      "Epoch: 9, Step: 1180, Loss: 0.0831698\n",
      "Epoch: 9, Step: 1190, Loss: 0.117942\n",
      "Epoch: 9, Step: 1200, Loss: 0.0145986\n",
      "Epoch: 9, Step: 1210, Loss: 0.212792\n",
      "Epoch: 9, Step: 1220, Loss: 0.334345\n",
      "Epoch: 9, Step: 1230, Loss: 0.491237\n",
      "Epoch: 9, Step: 1240, Loss: 1.26776\n",
      "Epoch: 9, Step: 1250, Loss: 0.340753\n",
      "Epoch: 9, Step: 1260, Loss: 0.0127336\n",
      "Epoch: 9, Step: 1270, Loss: 0.0207676\n",
      "Epoch: 9, Step: 1280, Loss: 0.0375958\n",
      "Epoch: 9, Step: 1290, Loss: 0.0490891\n",
      "Epoch: 9, Step: 1300, Loss: 0.289469\n",
      "Epoch: 9, Step: 1310, Loss: 0.610722\n",
      "Epoch: 9, Step: 1320, Loss: 0.27384\n",
      "Epoch: 9, Step: 1330, Loss: 0.0958488\n",
      "Epoch: 9, Step: 1340, Loss: 0.0492892\n",
      "Epoch: 9, Step: 1350, Loss: 0.0235175\n",
      "Epoch: 10, Step: 1000, Loss: 0.128145\n",
      "Epoch: 10, Step: 1010, Loss: 0.855727\n",
      "Epoch: 10, Step: 1020, Loss: 1.50599\n",
      "Epoch: 10, Step: 1030, Loss: 0.238396\n",
      "Epoch: 10, Step: 1040, Loss: 0.124346\n",
      "Epoch: 10, Step: 1050, Loss: 0.0101993\n",
      "Epoch: 10, Step: 1060, Loss: 0.0160144\n",
      "Epoch: 10, Step: 1070, Loss: 0.0122819\n",
      "Epoch: 10, Step: 1080, Loss: 0.0228045\n",
      "Epoch: 10, Step: 1090, Loss: 0.0467294\n",
      "Epoch: 10, Step: 1100, Loss: 0.0516657\n",
      "Epoch: 10, Step: 1110, Loss: 0.0141869\n",
      "Epoch: 10, Step: 1120, Loss: 0.130322\n",
      "Epoch: 10, Step: 1130, Loss: 0.40194\n",
      "Epoch: 10, Step: 1140, Loss: 0.106724\n",
      "Epoch: 10, Step: 1150, Loss: 0.18553\n",
      "Epoch: 10, Step: 1160, Loss: 0.0205442\n",
      "Epoch: 10, Step: 1170, Loss: 0.0224164\n",
      "Epoch: 10, Step: 1180, Loss: 0.023067\n",
      "Epoch: 10, Step: 1190, Loss: 0.0226477\n",
      "Epoch: 10, Step: 1200, Loss: 0.033023\n",
      "Epoch: 10, Step: 1210, Loss: 4.42056\n",
      "Epoch: 10, Step: 1220, Loss: 0.963052\n",
      "Epoch: 10, Step: 1230, Loss: 0.0235436\n",
      "Epoch: 10, Step: 1240, Loss: 0.0126547\n",
      "Epoch: 10, Step: 1250, Loss: 0.0135651\n",
      "Epoch: 10, Step: 1260, Loss: 0.0190639\n",
      "Epoch: 10, Step: 1270, Loss: 0.0124954\n",
      "Epoch: 10, Step: 1280, Loss: 0.0113022\n",
      "Epoch: 10, Step: 1290, Loss: 0.00902239\n",
      "Epoch: 10, Step: 1300, Loss: 0.0103682\n",
      "Epoch: 10, Step: 1310, Loss: 0.0121024\n",
      "Epoch: 10, Step: 1320, Loss: 0.0128135\n",
      "Epoch: 10, Step: 1330, Loss: 0.0167412\n",
      "Epoch: 10, Step: 1340, Loss: 0.0442277\n",
      "Epoch: 10, Step: 1350, Loss: 0.0318252\n",
      "Epoch: 10, Step: 1360, Loss: 0.00872453\n",
      "Epoch: 10, Step: 1370, Loss: 0.00686451\n",
      "Epoch: 10, Step: 1380, Loss: 0.00790534\n",
      "Epoch: 10, Step: 1390, Loss: 0.00736837\n",
      "Epoch: 10, Step: 1400, Loss: 0.00598315\n",
      "Epoch: 10, Step: 1410, Loss: 0.00774211\n",
      "Epoch: 10, Step: 1420, Loss: 0.00593032\n",
      "Epoch: 10, Step: 1430, Loss: 0.00618459\n",
      "Epoch: 10, Step: 1440, Loss: 0.00714076\n",
      "Epoch: 10, Step: 1450, Loss: 0.00679437\n",
      "Epoch: 11, Step: 1100, Loss: 0.00650526\n",
      "Epoch: 11, Step: 1110, Loss: 0.00599434\n",
      "Epoch: 11, Step: 1120, Loss: 0.00596203\n",
      "Epoch: 11, Step: 1130, Loss: 0.00591253\n",
      "Epoch: 11, Step: 1140, Loss: 0.00712907\n",
      "Epoch: 11, Step: 1150, Loss: 0.0593907\n",
      "Epoch: 11, Step: 1160, Loss: 0.197723\n",
      "Epoch: 11, Step: 1170, Loss: 5.20376\n",
      "Epoch: 11, Step: 1180, Loss: 0.0178056\n",
      "Epoch: 11, Step: 1190, Loss: 0.00606956\n",
      "Epoch: 11, Step: 1200, Loss: 0.0423989\n",
      "Epoch: 11, Step: 1210, Loss: 0.0751002\n",
      "Epoch: 11, Step: 1220, Loss: 0.0723629\n",
      "Epoch: 11, Step: 1230, Loss: 0.00957075\n",
      "Epoch: 11, Step: 1240, Loss: 0.135865\n",
      "Epoch: 11, Step: 1250, Loss: 0.0551754\n",
      "Epoch: 11, Step: 1260, Loss: 0.00586252\n",
      "Epoch: 11, Step: 1270, Loss: 0.0628438\n",
      "Epoch: 11, Step: 1280, Loss: 0.0134934\n",
      "Epoch: 11, Step: 1290, Loss: 0.02405\n",
      "Epoch: 11, Step: 1300, Loss: 0.0420239\n",
      "Epoch: 11, Step: 1310, Loss: 0.132449\n",
      "Epoch: 11, Step: 1320, Loss: 0.0560559\n",
      "Epoch: 11, Step: 1330, Loss: 0.142746\n",
      "Epoch: 11, Step: 1340, Loss: 0.0585688\n",
      "Epoch: 11, Step: 1350, Loss: 0.037132\n",
      "Epoch: 11, Step: 1360, Loss: 0.0256505\n",
      "Epoch: 11, Step: 1370, Loss: 0.0217615\n",
      "Epoch: 11, Step: 1380, Loss: 0.0226396\n",
      "Epoch: 11, Step: 1390, Loss: 0.0224016\n",
      "Epoch: 11, Step: 1400, Loss: 0.0230408\n",
      "Epoch: 11, Step: 1410, Loss: 0.0226389\n",
      "Epoch: 11, Step: 1420, Loss: 0.0296007\n",
      "Epoch: 11, Step: 1430, Loss: 0.0306697\n",
      "Epoch: 11, Step: 1440, Loss: 0.0411753\n",
      "Epoch: 11, Step: 1450, Loss: 0.0108661\n",
      "Epoch: 11, Step: 1460, Loss: 0.030754\n",
      "Epoch: 11, Step: 1470, Loss: 0.101369\n",
      "Epoch: 11, Step: 1480, Loss: 0.089908\n",
      "Epoch: 11, Step: 1490, Loss: 0.234424\n",
      "Epoch: 11, Step: 1500, Loss: 0.016953\n",
      "Epoch: 11, Step: 1510, Loss: 0.125611\n",
      "Epoch: 11, Step: 1520, Loss: 0.137596\n",
      "Epoch: 11, Step: 1530, Loss: 0.374581\n",
      "Epoch: 11, Step: 1540, Loss: 0.0379832\n",
      "Epoch: 11, Step: 1550, Loss: 0.182694\n",
      "Epoch: 12, Step: 1200, Loss: 0.34523\n",
      "Epoch: 12, Step: 1210, Loss: 0.12233\n",
      "Epoch: 12, Step: 1220, Loss: 0.0545296\n",
      "Epoch: 12, Step: 1230, Loss: 0.0110361\n",
      "Epoch: 12, Step: 1240, Loss: 0.0203332\n",
      "Epoch: 12, Step: 1250, Loss: 0.0339993\n",
      "Epoch: 12, Step: 1260, Loss: 0.549098\n",
      "Epoch: 12, Step: 1270, Loss: 0.478935\n",
      "Epoch: 12, Step: 1280, Loss: 0.521186\n",
      "Epoch: 12, Step: 1290, Loss: 0.920942\n",
      "Epoch: 12, Step: 1300, Loss: 0.166235\n",
      "Epoch: 12, Step: 1310, Loss: 0.646581\n",
      "Epoch: 12, Step: 1320, Loss: 0.22225\n",
      "Epoch: 12, Step: 1330, Loss: 0.0110562\n",
      "Epoch: 12, Step: 1340, Loss: 0.0282366\n",
      "Epoch: 12, Step: 1350, Loss: 0.102654\n",
      "Epoch: 12, Step: 1360, Loss: 0.172765\n",
      "Epoch: 12, Step: 1370, Loss: 0.985824\n",
      "Epoch: 12, Step: 1380, Loss: 0.349504\n",
      "Epoch: 12, Step: 1390, Loss: 2.19745\n",
      "Epoch: 12, Step: 1400, Loss: 0.518966\n",
      "Epoch: 12, Step: 1410, Loss: 0.892021\n",
      "Epoch: 12, Step: 1420, Loss: 0.0236483\n",
      "Epoch: 12, Step: 1430, Loss: 0.0116464\n",
      "Epoch: 12, Step: 1440, Loss: 0.454948\n",
      "Epoch: 12, Step: 1450, Loss: 1.52435\n",
      "Epoch: 12, Step: 1460, Loss: 0.30628\n",
      "Epoch: 12, Step: 1470, Loss: 0.120517\n",
      "Epoch: 12, Step: 1480, Loss: 0.00814822\n",
      "Epoch: 12, Step: 1490, Loss: 0.13265\n",
      "Epoch: 12, Step: 1500, Loss: 0.36021\n",
      "Epoch: 12, Step: 1510, Loss: 0.329814\n",
      "Epoch: 12, Step: 1520, Loss: 1.35595\n",
      "Epoch: 12, Step: 1530, Loss: 0.53179\n",
      "Epoch: 12, Step: 1540, Loss: 0.00596496\n",
      "Epoch: 12, Step: 1550, Loss: 0.0176764\n",
      "Epoch: 12, Step: 1560, Loss: 0.0332564\n",
      "Epoch: 12, Step: 1570, Loss: 0.041169\n",
      "Epoch: 12, Step: 1580, Loss: 0.1663\n",
      "Epoch: 12, Step: 1590, Loss: 0.578504\n",
      "Epoch: 12, Step: 1600, Loss: 0.39292\n",
      "Epoch: 12, Step: 1610, Loss: 0.113591\n",
      "Epoch: 12, Step: 1620, Loss: 0.0418734\n",
      "Epoch: 12, Step: 1630, Loss: 0.0163352\n",
      "Epoch: 12, Step: 1640, Loss: 0.0757261\n",
      "Epoch: 12, Step: 1650, Loss: 0.564316\n",
      "Epoch: 13, Step: 1300, Loss: 1.44893\n",
      "Epoch: 13, Step: 1310, Loss: 0.48061\n",
      "Epoch: 13, Step: 1320, Loss: 0.117797\n",
      "Epoch: 13, Step: 1330, Loss: 0.0127894\n",
      "Epoch: 13, Step: 1340, Loss: 0.00784113\n",
      "Epoch: 13, Step: 1350, Loss: 0.0033613\n",
      "Epoch: 13, Step: 1360, Loss: 0.0119388\n",
      "Epoch: 13, Step: 1370, Loss: 0.0353228\n",
      "Epoch: 13, Step: 1380, Loss: 0.0490759\n",
      "Epoch: 13, Step: 1390, Loss: 0.00448108\n",
      "Epoch: 13, Step: 1400, Loss: 0.0991218\n",
      "Epoch: 13, Step: 1410, Loss: 0.264831\n",
      "Epoch: 13, Step: 1420, Loss: 0.242393\n",
      "Epoch: 13, Step: 1430, Loss: 0.216303\n",
      "Epoch: 13, Step: 1440, Loss: 0.0166982\n",
      "Epoch: 13, Step: 1450, Loss: 0.0146256\n",
      "Epoch: 13, Step: 1460, Loss: 0.0119963\n",
      "Epoch: 13, Step: 1470, Loss: 0.0115009\n",
      "Epoch: 13, Step: 1480, Loss: 0.0184906\n",
      "Epoch: 13, Step: 1490, Loss: 2.94973\n",
      "Epoch: 13, Step: 1500, Loss: 2.60095\n",
      "Epoch: 13, Step: 1510, Loss: 0.0124997\n",
      "Epoch: 13, Step: 1520, Loss: 0.0050941\n",
      "Epoch: 13, Step: 1530, Loss: 0.00452485\n",
      "Epoch: 13, Step: 1540, Loss: 0.0108123\n",
      "Epoch: 13, Step: 1550, Loss: 0.00444826\n",
      "Epoch: 13, Step: 1560, Loss: 0.00675499\n",
      "Epoch: 13, Step: 1570, Loss: 0.00334486\n",
      "Epoch: 13, Step: 1580, Loss: 0.00508148\n",
      "Epoch: 13, Step: 1590, Loss: 0.00495334\n",
      "Epoch: 13, Step: 1600, Loss: 0.00217343\n",
      "Epoch: 13, Step: 1610, Loss: 0.0115796\n",
      "Epoch: 13, Step: 1620, Loss: 0.0456331\n",
      "Epoch: 13, Step: 1630, Loss: 0.0395699\n",
      "Epoch: 13, Step: 1640, Loss: 0.00255092\n",
      "Epoch: 13, Step: 1650, Loss: 0.00184102\n",
      "Epoch: 13, Step: 1660, Loss: 0.00199452\n",
      "Epoch: 13, Step: 1670, Loss: 0.00235305\n",
      "Epoch: 13, Step: 1680, Loss: 0.00146473\n",
      "Epoch: 13, Step: 1690, Loss: 0.00279156\n",
      "Epoch: 13, Step: 1700, Loss: 0.00150532\n",
      "Epoch: 13, Step: 1710, Loss: 0.00145599\n",
      "Epoch: 13, Step: 1720, Loss: 0.00196328\n",
      "Epoch: 13, Step: 1730, Loss: 0.00186134\n",
      "Epoch: 13, Step: 1740, Loss: 0.00174343\n",
      "Epoch: 13, Step: 1750, Loss: 0.0014369\n",
      "Epoch: 14, Step: 1400, Loss: 0.00123544\n",
      "Epoch: 14, Step: 1410, Loss: 0.0014968\n",
      "Epoch: 14, Step: 1420, Loss: 0.00262114\n",
      "Epoch: 14, Step: 1430, Loss: 0.037241\n",
      "Epoch: 14, Step: 1440, Loss: 0.0331712\n",
      "Epoch: 14, Step: 1450, Loss: 5.07335\n",
      "Epoch: 14, Step: 1460, Loss: 0.475078\n",
      "Epoch: 14, Step: 1470, Loss: 0.00357249\n",
      "Epoch: 14, Step: 1480, Loss: 0.0197104\n",
      "Epoch: 14, Step: 1490, Loss: 0.0576863\n",
      "Epoch: 14, Step: 1500, Loss: 0.0669604\n",
      "Epoch: 14, Step: 1510, Loss: 0.00937629\n",
      "Epoch: 14, Step: 1520, Loss: 0.101991\n",
      "Epoch: 14, Step: 1530, Loss: 0.0795467\n",
      "Epoch: 14, Step: 1540, Loss: 0.00236027\n",
      "Epoch: 14, Step: 1550, Loss: 0.0537883\n",
      "Epoch: 14, Step: 1560, Loss: 0.0123297\n",
      "Epoch: 14, Step: 1570, Loss: 0.0157282\n",
      "Epoch: 14, Step: 1580, Loss: 0.00951615\n",
      "Epoch: 14, Step: 1590, Loss: 0.158959\n",
      "Epoch: 14, Step: 1600, Loss: 0.0383406\n",
      "Epoch: 14, Step: 1610, Loss: 0.130586\n",
      "Epoch: 14, Step: 1620, Loss: 0.0728775\n",
      "Epoch: 14, Step: 1630, Loss: 0.0344589\n",
      "Epoch: 14, Step: 1640, Loss: 0.0214597\n",
      "Epoch: 14, Step: 1650, Loss: 0.0216484\n",
      "Epoch: 14, Step: 1660, Loss: 0.022072\n",
      "Epoch: 14, Step: 1670, Loss: 0.0235301\n",
      "Epoch: 14, Step: 1680, Loss: 0.0234506\n",
      "Epoch: 14, Step: 1690, Loss: 0.0222859\n",
      "Epoch: 14, Step: 1700, Loss: 0.0222723\n",
      "Epoch: 14, Step: 1710, Loss: 0.0377595\n",
      "Epoch: 14, Step: 1720, Loss: 0.038339\n",
      "Epoch: 14, Step: 1730, Loss: 0.0164892\n",
      "Epoch: 14, Step: 1740, Loss: 0.0163153\n",
      "Epoch: 14, Step: 1750, Loss: 0.0748168\n",
      "Epoch: 14, Step: 1760, Loss: 0.034328\n",
      "Epoch: 14, Step: 1770, Loss: 0.325782\n",
      "Epoch: 14, Step: 1780, Loss: 0.0116768\n",
      "Epoch: 14, Step: 1790, Loss: 0.078656\n",
      "Epoch: 14, Step: 1800, Loss: 0.0853418\n",
      "Epoch: 14, Step: 1810, Loss: 0.382396\n",
      "Epoch: 14, Step: 1820, Loss: 0.0499424\n",
      "Epoch: 14, Step: 1830, Loss: 0.129916\n",
      "Epoch: 14, Step: 1840, Loss: 0.295599\n",
      "Epoch: 14, Step: 1850, Loss: 0.221223\n",
      "Epoch: 15, Step: 1500, Loss: 0.0331758\n",
      "Epoch: 15, Step: 1510, Loss: 0.0262397\n",
      "Epoch: 15, Step: 1520, Loss: 0.0118635\n",
      "Epoch: 15, Step: 1530, Loss: 0.050053\n",
      "Epoch: 15, Step: 1540, Loss: 0.203282\n",
      "Epoch: 15, Step: 1550, Loss: 0.755705\n",
      "Epoch: 15, Step: 1560, Loss: 0.322723\n",
      "Epoch: 15, Step: 1570, Loss: 0.966133\n",
      "Epoch: 15, Step: 1580, Loss: 0.373336\n",
      "Epoch: 15, Step: 1590, Loss: 0.516524\n",
      "Epoch: 15, Step: 1600, Loss: 0.325727\n",
      "Epoch: 15, Step: 1610, Loss: 0.0130083\n",
      "Epoch: 15, Step: 1620, Loss: 0.0224629\n",
      "Epoch: 15, Step: 1630, Loss: 0.0731269\n",
      "Epoch: 15, Step: 1640, Loss: 0.174626\n",
      "Epoch: 15, Step: 1650, Loss: 0.916133\n",
      "Epoch: 15, Step: 1660, Loss: 0.177819\n",
      "Epoch: 15, Step: 1670, Loss: 2.16691\n",
      "Epoch: 15, Step: 1680, Loss: 0.576419\n",
      "Epoch: 15, Step: 1690, Loss: 1.06641\n",
      "Epoch: 15, Step: 1700, Loss: 0.0993708\n",
      "Epoch: 15, Step: 1710, Loss: 0.00503326\n",
      "Epoch: 15, Step: 1720, Loss: 0.196763\n",
      "Epoch: 15, Step: 1730, Loss: 1.3244\n",
      "Epoch: 15, Step: 1740, Loss: 0.635685\n",
      "Epoch: 15, Step: 1750, Loss: 0.124205\n",
      "Epoch: 15, Step: 1760, Loss: 0.0306165\n",
      "Epoch: 15, Step: 1770, Loss: 0.0818826\n",
      "Epoch: 15, Step: 1780, Loss: 0.375284\n",
      "Epoch: 15, Step: 1790, Loss: 0.229634\n",
      "Epoch: 15, Step: 1800, Loss: 1.19427\n",
      "Epoch: 15, Step: 1810, Loss: 0.74769\n",
      "Epoch: 15, Step: 1820, Loss: 0.0466874\n",
      "Epoch: 15, Step: 1830, Loss: 0.012073\n",
      "Epoch: 15, Step: 1840, Loss: 0.0211542\n",
      "Epoch: 15, Step: 1850, Loss: 0.0279036\n",
      "Epoch: 15, Step: 1860, Loss: 0.0615827\n",
      "Epoch: 15, Step: 1870, Loss: 0.583657\n",
      "Epoch: 15, Step: 1880, Loss: 0.521837\n",
      "Epoch: 15, Step: 1890, Loss: 0.149918\n",
      "Epoch: 15, Step: 1900, Loss: 0.0423974\n",
      "Epoch: 15, Step: 1910, Loss: 0.0228874\n",
      "Epoch: 15, Step: 1920, Loss: 0.0430273\n",
      "Epoch: 15, Step: 1930, Loss: 0.333895\n",
      "Epoch: 15, Step: 1940, Loss: 1.41265\n",
      "Epoch: 15, Step: 1950, Loss: 0.734317\n",
      "Epoch: 16, Step: 1600, Loss: 0.0932302\n",
      "Epoch: 16, Step: 1610, Loss: 0.0454771\n",
      "Epoch: 16, Step: 1620, Loss: 0.0052032\n",
      "Epoch: 16, Step: 1630, Loss: 0.00146476\n",
      "Epoch: 16, Step: 1640, Loss: 0.0103982\n",
      "Epoch: 16, Step: 1650, Loss: 0.0253869\n",
      "Epoch: 16, Step: 1660, Loss: 0.0582451\n",
      "Epoch: 16, Step: 1670, Loss: 0.00194384\n",
      "Epoch: 16, Step: 1680, Loss: 0.0524577\n",
      "Epoch: 16, Step: 1690, Loss: 0.124195\n",
      "Epoch: 16, Step: 1700, Loss: 0.363948\n",
      "Epoch: 16, Step: 1710, Loss: 0.206065\n",
      "Epoch: 16, Step: 1720, Loss: 0.0125648\n",
      "Epoch: 16, Step: 1730, Loss: 0.00751974\n",
      "Epoch: 16, Step: 1740, Loss: 0.00737692\n",
      "Epoch: 16, Step: 1750, Loss: 0.00743447\n",
      "Epoch: 16, Step: 1760, Loss: 0.011077\n",
      "Epoch: 16, Step: 1770, Loss: 1.39943\n",
      "Epoch: 16, Step: 1780, Loss: 4.11363\n",
      "Epoch: 16, Step: 1790, Loss: 0.0151701\n",
      "Epoch: 16, Step: 1800, Loss: 0.00502144\n",
      "Epoch: 16, Step: 1810, Loss: 0.00620449\n",
      "Epoch: 16, Step: 1820, Loss: 0.012761\n",
      "Epoch: 16, Step: 1830, Loss: 0.00652451\n",
      "Epoch: 16, Step: 1840, Loss: 0.00651617\n",
      "Epoch: 16, Step: 1850, Loss: 0.00270484\n",
      "Epoch: 16, Step: 1860, Loss: 0.00235971\n",
      "Epoch: 16, Step: 1870, Loss: 0.00317132\n",
      "Epoch: 16, Step: 1880, Loss: 0.000966714\n",
      "Epoch: 16, Step: 1890, Loss: 0.005897\n",
      "Epoch: 16, Step: 1900, Loss: 0.0418223\n",
      "Epoch: 16, Step: 1910, Loss: 0.0437091\n",
      "Epoch: 16, Step: 1920, Loss: 0.0105082\n",
      "Epoch: 16, Step: 1930, Loss: 0.000730963\n",
      "Epoch: 16, Step: 1940, Loss: 0.00128156\n",
      "Epoch: 16, Step: 1950, Loss: 0.00160824\n",
      "Epoch: 16, Step: 1960, Loss: 0.000494341\n",
      "Epoch: 16, Step: 1970, Loss: 0.00117609\n",
      "Epoch: 16, Step: 1980, Loss: 0.00111462\n",
      "Epoch: 16, Step: 1990, Loss: 0.000819039\n",
      "Epoch: 16, Step: 2000, Loss: 0.00264419\n",
      "Epoch: 16, Step: 2010, Loss: 0.00157698\n",
      "Epoch: 16, Step: 2020, Loss: 0.00319141\n",
      "Epoch: 16, Step: 2030, Loss: 0.00260994\n",
      "Epoch: 16, Step: 2040, Loss: 0.00293901\n",
      "Epoch: 16, Step: 2050, Loss: 0.00257835\n",
      "Epoch: 17, Step: 1700, Loss: 0.00326116\n",
      "Epoch: 17, Step: 1710, Loss: 0.0363355\n",
      "Epoch: 17, Step: 1720, Loss: 0.0375084\n",
      "Epoch: 17, Step: 1730, Loss: 3.65202\n",
      "Epoch: 17, Step: 1740, Loss: 1.87536\n",
      "Epoch: 17, Step: 1750, Loss: 0.00355113\n",
      "Epoch: 17, Step: 1760, Loss: 0.00708473\n",
      "Epoch: 17, Step: 1770, Loss: 0.0631669\n",
      "Epoch: 17, Step: 1780, Loss: 0.0804284\n",
      "Epoch: 17, Step: 1790, Loss: 0.0293918\n",
      "Epoch: 17, Step: 1800, Loss: 0.0687006\n",
      "Epoch: 17, Step: 1810, Loss: 0.0950459\n",
      "Epoch: 17, Step: 1820, Loss: 0.00333558\n",
      "Epoch: 17, Step: 1830, Loss: 0.0373795\n",
      "Epoch: 17, Step: 1840, Loss: 0.0188446\n",
      "Epoch: 17, Step: 1850, Loss: 0.0153933\n",
      "Epoch: 17, Step: 1860, Loss: 0.00581596\n",
      "Epoch: 17, Step: 1870, Loss: 0.134542\n",
      "Epoch: 17, Step: 1880, Loss: 0.0694708\n",
      "Epoch: 17, Step: 1890, Loss: 0.103069\n",
      "Epoch: 17, Step: 1900, Loss: 0.0957223\n",
      "Epoch: 17, Step: 1910, Loss: 0.0302142\n",
      "Epoch: 17, Step: 1920, Loss: 0.0256659\n",
      "Epoch: 17, Step: 1930, Loss: 0.0251001\n",
      "Epoch: 17, Step: 1940, Loss: 0.0203899\n",
      "Epoch: 17, Step: 1950, Loss: 0.0173483\n",
      "Epoch: 17, Step: 1960, Loss: 0.0163405\n",
      "Epoch: 17, Step: 1970, Loss: 0.0161821\n",
      "Epoch: 17, Step: 1980, Loss: 0.0128743\n",
      "Epoch: 17, Step: 1990, Loss: 0.0259835\n",
      "Epoch: 17, Step: 2000, Loss: 0.0214774\n",
      "Epoch: 17, Step: 2010, Loss: 0.016585\n",
      "Epoch: 17, Step: 2020, Loss: 0.0091365\n",
      "Epoch: 17, Step: 2030, Loss: 0.045041\n",
      "Epoch: 17, Step: 2040, Loss: 0.057236\n",
      "Epoch: 17, Step: 2050, Loss: 0.314748\n",
      "Epoch: 17, Step: 2060, Loss: 0.0145051\n",
      "Epoch: 17, Step: 2070, Loss: 0.0580575\n",
      "Epoch: 17, Step: 2080, Loss: 0.0988285\n",
      "Epoch: 17, Step: 2090, Loss: 0.369877\n",
      "Epoch: 17, Step: 2100, Loss: 0.115587\n",
      "Epoch: 17, Step: 2110, Loss: 0.0612007\n",
      "Epoch: 17, Step: 2120, Loss: 0.265419\n",
      "Epoch: 17, Step: 2130, Loss: 0.278055\n",
      "Epoch: 17, Step: 2140, Loss: 0.0116049\n",
      "Epoch: 17, Step: 2150, Loss: 0.0448018\n",
      "Epoch: 18, Step: 1800, Loss: 0.00795993\n",
      "Epoch: 18, Step: 1810, Loss: 0.0513836\n",
      "Epoch: 18, Step: 1820, Loss: 0.0554576\n",
      "Epoch: 18, Step: 1830, Loss: 0.848536\n",
      "Epoch: 18, Step: 1840, Loss: 0.172818\n",
      "Epoch: 18, Step: 1850, Loss: 0.947857\n",
      "Epoch: 18, Step: 1860, Loss: 0.632048\n",
      "Epoch: 18, Step: 1870, Loss: 0.372247\n",
      "Epoch: 18, Step: 1880, Loss: 0.427303\n",
      "Epoch: 18, Step: 1890, Loss: 0.0276619\n",
      "Epoch: 18, Step: 1900, Loss: 0.0211734\n",
      "Epoch: 18, Step: 1910, Loss: 0.0401979\n",
      "Epoch: 18, Step: 1920, Loss: 0.182543\n",
      "Epoch: 18, Step: 1930, Loss: 0.71863\n",
      "Epoch: 18, Step: 1940, Loss: 0.401281\n",
      "Epoch: 18, Step: 1950, Loss: 1.74974\n",
      "Epoch: 18, Step: 1960, Loss: 0.785558\n",
      "Epoch: 18, Step: 1970, Loss: 1.12074\n",
      "Epoch: 18, Step: 1980, Loss: 0.267477\n",
      "Epoch: 18, Step: 1990, Loss: 0.00458397\n",
      "Epoch: 18, Step: 2000, Loss: 0.0816489\n",
      "Epoch: 18, Step: 2010, Loss: 1.1485\n",
      "Epoch: 18, Step: 2020, Loss: 0.926558\n",
      "Epoch: 18, Step: 2030, Loss: 0.0879067\n",
      "Epoch: 18, Step: 2040, Loss: 0.0618968\n",
      "Epoch: 18, Step: 2050, Loss: 0.0446459\n",
      "Epoch: 18, Step: 2060, Loss: 0.412509\n",
      "Epoch: 18, Step: 2070, Loss: 0.155052\n",
      "Epoch: 18, Step: 2080, Loss: 0.953834\n",
      "Epoch: 18, Step: 2090, Loss: 0.896485\n",
      "Epoch: 18, Step: 2100, Loss: 0.166618\n",
      "Epoch: 18, Step: 2110, Loss: 0.00754876\n",
      "Epoch: 18, Step: 2120, Loss: 0.0167575\n",
      "Epoch: 18, Step: 2130, Loss: 0.0253177\n",
      "Epoch: 18, Step: 2140, Loss: 0.0348716\n",
      "Epoch: 18, Step: 2150, Loss: 0.496305\n",
      "Epoch: 18, Step: 2160, Loss: 0.614296\n",
      "Epoch: 18, Step: 2170, Loss: 0.20622\n",
      "Epoch: 18, Step: 2180, Loss: 0.0455392\n",
      "Epoch: 18, Step: 2190, Loss: 0.0350249\n",
      "Epoch: 18, Step: 2200, Loss: 0.0188533\n",
      "Epoch: 18, Step: 2210, Loss: 0.165962\n",
      "Epoch: 18, Step: 2220, Loss: 1.2349\n",
      "Epoch: 18, Step: 2230, Loss: 1.10782\n",
      "Epoch: 18, Step: 2240, Loss: 0.0496967\n",
      "Epoch: 18, Step: 2250, Loss: 0.0864731\n",
      "Epoch: 19, Step: 1900, Loss: 0.00268131\n",
      "Epoch: 19, Step: 1910, Loss: 0.00723151\n",
      "Epoch: 19, Step: 1920, Loss: 0.00873491\n",
      "Epoch: 19, Step: 1930, Loss: 0.0291774\n",
      "Epoch: 19, Step: 1940, Loss: 0.0701261\n",
      "Epoch: 19, Step: 1950, Loss: 0.0141795\n",
      "Epoch: 19, Step: 1960, Loss: 0.0166166\n",
      "Epoch: 19, Step: 1970, Loss: 0.116638\n",
      "Epoch: 19, Step: 1980, Loss: 0.401886\n",
      "Epoch: 19, Step: 1990, Loss: 0.215667\n",
      "Epoch: 19, Step: 2000, Loss: 0.0153204\n",
      "Epoch: 19, Step: 2010, Loss: 0.010217\n",
      "Epoch: 19, Step: 2020, Loss: 0.0113979\n",
      "Epoch: 19, Step: 2030, Loss: 0.0124642\n",
      "Epoch: 19, Step: 2040, Loss: 0.0116976\n",
      "Epoch: 19, Step: 2050, Loss: 0.352959\n",
      "Epoch: 19, Step: 2060, Loss: 5.03628\n",
      "Epoch: 19, Step: 2070, Loss: 0.0594675\n",
      "Epoch: 19, Step: 2080, Loss: 0.0115721\n",
      "Epoch: 19, Step: 2090, Loss: 0.00627686\n",
      "Epoch: 19, Step: 2100, Loss: 0.0131305\n",
      "Epoch: 19, Step: 2110, Loss: 0.00958355\n",
      "Epoch: 19, Step: 2120, Loss: 0.00795199\n",
      "Epoch: 19, Step: 2130, Loss: 0.00153867\n",
      "Epoch: 19, Step: 2140, Loss: 0.00191586\n",
      "Epoch: 19, Step: 2150, Loss: 0.00167916\n",
      "Epoch: 19, Step: 2160, Loss: 0.00128692\n",
      "Epoch: 19, Step: 2170, Loss: 0.00333326\n",
      "Epoch: 19, Step: 2180, Loss: 0.0356163\n",
      "Epoch: 19, Step: 2190, Loss: 0.0407347\n",
      "Epoch: 19, Step: 2200, Loss: 0.0166139\n",
      "Epoch: 19, Step: 2210, Loss: 0.00157101\n",
      "Epoch: 19, Step: 2220, Loss: 0.00132999\n",
      "Epoch: 19, Step: 2230, Loss: 0.00260962\n",
      "Epoch: 19, Step: 2240, Loss: 0.00199741\n",
      "Epoch: 19, Step: 2250, Loss: 0.00173215\n",
      "Epoch: 19, Step: 2260, Loss: 0.00341912\n",
      "Epoch: 19, Step: 2270, Loss: 0.00159946\n",
      "Epoch: 19, Step: 2280, Loss: 0.00313239\n",
      "Epoch: 19, Step: 2290, Loss: 0.00228074\n",
      "Epoch: 19, Step: 2300, Loss: 0.00215741\n",
      "Epoch: 19, Step: 2310, Loss: 0.00212178\n",
      "Epoch: 19, Step: 2320, Loss: 0.00200251\n",
      "Epoch: 19, Step: 2330, Loss: 0.000861557\n",
      "Epoch: 19, Step: 2340, Loss: 0.00135943\n",
      "Epoch: 19, Step: 2350, Loss: 0.00654531\n",
      "Epoch: 20, Step: 2000, Loss: 0.0606967\n",
      "Epoch: 20, Step: 2010, Loss: 2.43699\n",
      "Epoch: 20, Step: 2020, Loss: 3.01369\n",
      "Epoch: 20, Step: 2030, Loss: 0.00374198\n",
      "Epoch: 20, Step: 2040, Loss: 0.00120197\n",
      "Epoch: 20, Step: 2050, Loss: 0.060123\n",
      "Epoch: 20, Step: 2060, Loss: 0.0681849\n",
      "Epoch: 20, Step: 2070, Loss: 0.0580373\n",
      "Epoch: 20, Step: 2080, Loss: 0.0425001\n",
      "Epoch: 20, Step: 2090, Loss: 0.107426\n",
      "Epoch: 20, Step: 2100, Loss: 0.00513819\n",
      "Epoch: 20, Step: 2110, Loss: 0.0238315\n",
      "Epoch: 20, Step: 2120, Loss: 0.0234487\n",
      "Epoch: 20, Step: 2130, Loss: 0.0172285\n",
      "Epoch: 20, Step: 2140, Loss: 0.0095001\n",
      "Epoch: 20, Step: 2150, Loss: 0.0932234\n",
      "Epoch: 20, Step: 2160, Loss: 0.095904\n",
      "Epoch: 20, Step: 2170, Loss: 0.0899049\n",
      "Epoch: 20, Step: 2180, Loss: 0.136436\n",
      "Epoch: 20, Step: 2190, Loss: 0.0139335\n",
      "Epoch: 20, Step: 2200, Loss: 0.0251579\n",
      "Epoch: 20, Step: 2210, Loss: 0.0152492\n",
      "Epoch: 20, Step: 2220, Loss: 0.0123747\n",
      "Epoch: 20, Step: 2230, Loss: 0.0123865\n",
      "Epoch: 20, Step: 2240, Loss: 0.0123367\n",
      "Epoch: 20, Step: 2250, Loss: 0.0139153\n",
      "Epoch: 20, Step: 2260, Loss: 0.0150622\n",
      "Epoch: 20, Step: 2270, Loss: 0.0278363\n",
      "Epoch: 20, Step: 2280, Loss: 0.021611\n",
      "Epoch: 20, Step: 2290, Loss: 0.0236478\n",
      "Epoch: 20, Step: 2300, Loss: 0.00492324\n",
      "Epoch: 20, Step: 2310, Loss: 0.0402722\n",
      "Epoch: 20, Step: 2320, Loss: 0.0879124\n",
      "Epoch: 20, Step: 2330, Loss: 0.256986\n",
      "Epoch: 20, Step: 2340, Loss: 0.0553892\n",
      "Epoch: 20, Step: 2350, Loss: 0.0463709\n",
      "Epoch: 20, Step: 2360, Loss: 0.11586\n",
      "Epoch: 20, Step: 2370, Loss: 0.283093\n",
      "Epoch: 20, Step: 2380, Loss: 0.224347\n",
      "Epoch: 20, Step: 2390, Loss: 0.02081\n",
      "Epoch: 20, Step: 2400, Loss: 0.240488\n",
      "Epoch: 20, Step: 2410, Loss: 0.307041\n",
      "Epoch: 20, Step: 2420, Loss: 0.0204725\n",
      "Epoch: 20, Step: 2430, Loss: 0.041291\n",
      "Epoch: 20, Step: 2440, Loss: 0.00482155\n",
      "Epoch: 20, Step: 2450, Loss: 0.0525324\n",
      "Epoch: 21, Step: 2100, Loss: 0.020974\n",
      "Epoch: 21, Step: 2110, Loss: 0.7803\n",
      "Epoch: 21, Step: 2120, Loss: 0.213925\n",
      "Epoch: 21, Step: 2130, Loss: 0.792794\n",
      "Epoch: 21, Step: 2140, Loss: 0.771209\n",
      "Epoch: 21, Step: 2150, Loss: 0.255103\n",
      "Epoch: 21, Step: 2160, Loss: 0.576676\n",
      "Epoch: 21, Step: 2170, Loss: 0.0820807\n",
      "Epoch: 21, Step: 2180, Loss: 0.0115842\n",
      "Epoch: 21, Step: 2190, Loss: 0.0235018\n",
      "Epoch: 21, Step: 2200, Loss: 0.168416\n",
      "Epoch: 21, Step: 2210, Loss: 0.391256\n",
      "Epoch: 21, Step: 2220, Loss: 0.750737\n",
      "Epoch: 21, Step: 2230, Loss: 1.1797\n",
      "Epoch: 21, Step: 2240, Loss: 1.32905\n",
      "Epoch: 21, Step: 2250, Loss: 0.925916\n",
      "Epoch: 21, Step: 2260, Loss: 0.464297\n",
      "Epoch: 21, Step: 2270, Loss: 0.00115815\n",
      "Epoch: 21, Step: 2280, Loss: 0.0338015\n",
      "Epoch: 21, Step: 2290, Loss: 0.899807\n",
      "Epoch: 21, Step: 2300, Loss: 1.2869\n",
      "Epoch: 21, Step: 2310, Loss: 0.0565622\n",
      "Epoch: 21, Step: 2320, Loss: 0.0872734\n",
      "Epoch: 21, Step: 2330, Loss: 0.00714916\n",
      "Epoch: 21, Step: 2340, Loss: 0.285067\n",
      "Epoch: 21, Step: 2350, Loss: 0.24231\n",
      "Epoch: 21, Step: 2360, Loss: 0.708339\n",
      "Epoch: 21, Step: 2370, Loss: 1.07568\n",
      "Epoch: 21, Step: 2380, Loss: 0.268266\n",
      "Epoch: 21, Step: 2390, Loss: 0.00350493\n",
      "Epoch: 21, Step: 2400, Loss: 0.0123794\n",
      "Epoch: 21, Step: 2410, Loss: 0.0232447\n",
      "Epoch: 21, Step: 2420, Loss: 0.0303638\n",
      "Epoch: 21, Step: 2430, Loss: 0.371326\n",
      "Epoch: 21, Step: 2440, Loss: 0.620827\n",
      "Epoch: 21, Step: 2450, Loss: 0.233567\n",
      "Epoch: 21, Step: 2460, Loss: 0.0660264\n",
      "Epoch: 21, Step: 2470, Loss: 0.0363522\n",
      "Epoch: 21, Step: 2480, Loss: 0.0181245\n",
      "Epoch: 21, Step: 2490, Loss: 0.134781\n",
      "Epoch: 21, Step: 2500, Loss: 1.02159\n",
      "Epoch: 21, Step: 2510, Loss: 1.43356\n",
      "Epoch: 21, Step: 2520, Loss: 0.104496\n",
      "Epoch: 21, Step: 2530, Loss: 0.109295\n",
      "Epoch: 21, Step: 2540, Loss: 0.00120976\n",
      "Epoch: 21, Step: 2550, Loss: 0.00772949\n",
      "Epoch: 22, Step: 2200, Loss: 0.00814043\n",
      "Epoch: 22, Step: 2210, Loss: 0.0178632\n",
      "Epoch: 22, Step: 2220, Loss: 0.0464394\n",
      "Epoch: 22, Step: 2230, Loss: 0.0350951\n",
      "Epoch: 22, Step: 2240, Loss: 0.00579164\n",
      "Epoch: 22, Step: 2250, Loss: 0.123939\n",
      "Epoch: 22, Step: 2260, Loss: 0.419071\n",
      "Epoch: 22, Step: 2270, Loss: 0.109148\n",
      "Epoch: 22, Step: 2280, Loss: 0.140348\n",
      "Epoch: 22, Step: 2290, Loss: 0.0124247\n",
      "Epoch: 22, Step: 2300, Loss: 0.0127796\n",
      "Epoch: 22, Step: 2310, Loss: 0.0130515\n",
      "Epoch: 22, Step: 2320, Loss: 0.0128227\n",
      "Epoch: 22, Step: 2330, Loss: 0.036568\n",
      "Epoch: 22, Step: 2340, Loss: 5.08842\n",
      "Epoch: 22, Step: 2350, Loss: 0.260443\n",
      "Epoch: 22, Step: 2360, Loss: 0.0146589\n",
      "Epoch: 22, Step: 2370, Loss: 0.00696287\n",
      "Epoch: 22, Step: 2380, Loss: 0.0127219\n",
      "Epoch: 22, Step: 2390, Loss: 0.0146582\n",
      "Epoch: 22, Step: 2400, Loss: 0.00670526\n",
      "Epoch: 22, Step: 2410, Loss: 0.00320612\n",
      "Epoch: 22, Step: 2420, Loss: 0.00252504\n",
      "Epoch: 22, Step: 2430, Loss: 0.00274868\n",
      "Epoch: 22, Step: 2440, Loss: 0.00472331\n",
      "Epoch: 22, Step: 2450, Loss: 0.00612815\n",
      "Epoch: 22, Step: 2460, Loss: 0.0147272\n",
      "Epoch: 22, Step: 2470, Loss: 0.0353576\n",
      "Epoch: 22, Step: 2480, Loss: 0.0187168\n",
      "Epoch: 22, Step: 2490, Loss: 0.00339025\n",
      "Epoch: 22, Step: 2500, Loss: 0.000983218\n",
      "Epoch: 22, Step: 2510, Loss: 0.00220094\n",
      "Epoch: 22, Step: 2520, Loss: 0.00188282\n",
      "Epoch: 22, Step: 2530, Loss: 0.000422885\n",
      "Epoch: 22, Step: 2540, Loss: 0.00270257\n",
      "Epoch: 22, Step: 2550, Loss: 0.000393548\n",
      "Epoch: 22, Step: 2560, Loss: 0.00102155\n",
      "Epoch: 22, Step: 2570, Loss: 0.00141904\n",
      "Epoch: 22, Step: 2580, Loss: 0.00150534\n",
      "Epoch: 22, Step: 2590, Loss: 0.000998239\n",
      "Epoch: 22, Step: 2600, Loss: 0.000802664\n",
      "Epoch: 22, Step: 2610, Loss: 0.000598476\n",
      "Epoch: 22, Step: 2620, Loss: 0.000888671\n",
      "Epoch: 22, Step: 2630, Loss: 0.00397222\n",
      "Epoch: 22, Step: 2640, Loss: 0.0615743\n",
      "Epoch: 22, Step: 2650, Loss: 1.0522\n",
      "Epoch: 23, Step: 2300, Loss: 4.33118\n",
      "Epoch: 23, Step: 2310, Loss: 0.00577952\n",
      "Epoch: 23, Step: 2320, Loss: 0.000870738\n",
      "Epoch: 23, Step: 2330, Loss: 0.0517765\n",
      "Epoch: 23, Step: 2340, Loss: 0.0730058\n",
      "Epoch: 23, Step: 2350, Loss: 0.0748541\n",
      "Epoch: 23, Step: 2360, Loss: 0.0156573\n",
      "Epoch: 23, Step: 2370, Loss: 0.132771\n",
      "Epoch: 23, Step: 2380, Loss: 0.023219\n",
      "Epoch: 23, Step: 2390, Loss: 0.00791507\n",
      "Epoch: 23, Step: 2400, Loss: 0.0498769\n",
      "Epoch: 23, Step: 2410, Loss: 0.0106652\n",
      "Epoch: 23, Step: 2420, Loss: 0.0177526\n",
      "Epoch: 23, Step: 2430, Loss: 0.0556234\n",
      "Epoch: 23, Step: 2440, Loss: 0.107613\n",
      "Epoch: 23, Step: 2450, Loss: 0.0767426\n",
      "Epoch: 23, Step: 2460, Loss: 0.137444\n",
      "Epoch: 23, Step: 2470, Loss: 0.0340659\n",
      "Epoch: 23, Step: 2480, Loss: 0.0315395\n",
      "Epoch: 23, Step: 2490, Loss: 0.0173694\n",
      "Epoch: 23, Step: 2500, Loss: 0.0154322\n",
      "Epoch: 23, Step: 2510, Loss: 0.017618\n",
      "Epoch: 23, Step: 2520, Loss: 0.0187349\n",
      "Epoch: 23, Step: 2530, Loss: 0.0191024\n",
      "Epoch: 23, Step: 2540, Loss: 0.0186892\n",
      "Epoch: 23, Step: 2550, Loss: 0.0313383\n",
      "Epoch: 23, Step: 2560, Loss: 0.0247928\n",
      "Epoch: 23, Step: 2570, Loss: 0.032918\n",
      "Epoch: 23, Step: 2580, Loss: 0.0042134\n",
      "Epoch: 23, Step: 2590, Loss: 0.0319294\n",
      "Epoch: 23, Step: 2600, Loss: 0.0974727\n",
      "Epoch: 23, Step: 2610, Loss: 0.159523\n",
      "Epoch: 23, Step: 2620, Loss: 0.146261\n",
      "Epoch: 23, Step: 2630, Loss: 0.0303007\n",
      "Epoch: 23, Step: 2640, Loss: 0.123718\n",
      "Epoch: 23, Step: 2650, Loss: 0.189683\n",
      "Epoch: 23, Step: 2660, Loss: 0.344241\n",
      "Epoch: 23, Step: 2670, Loss: 0.0175677\n",
      "Epoch: 23, Step: 2680, Loss: 0.210299\n",
      "Epoch: 23, Step: 2690, Loss: 0.361637\n",
      "Epoch: 23, Step: 2700, Loss: 0.0461392\n",
      "Epoch: 23, Step: 2710, Loss: 0.0520997\n",
      "Epoch: 23, Step: 2720, Loss: 0.0054078\n",
      "Epoch: 23, Step: 2730, Loss: 0.0301671\n",
      "Epoch: 23, Step: 2740, Loss: 0.0219367\n",
      "Epoch: 23, Step: 2750, Loss: 0.716619\n",
      "Epoch: 24, Step: 2400, Loss: 0.331349\n",
      "Epoch: 24, Step: 2410, Loss: 0.646546\n",
      "Epoch: 24, Step: 2420, Loss: 0.851673\n",
      "Epoch: 24, Step: 2430, Loss: 0.128565\n",
      "Epoch: 24, Step: 2440, Loss: 0.648197\n",
      "Epoch: 24, Step: 2450, Loss: 0.162446\n",
      "Epoch: 24, Step: 2460, Loss: 0.00763192\n",
      "Epoch: 24, Step: 2470, Loss: 0.0269584\n",
      "Epoch: 24, Step: 2480, Loss: 0.144493\n",
      "Epoch: 24, Step: 2490, Loss: 0.210952\n",
      "Epoch: 24, Step: 2500, Loss: 0.92526\n",
      "Epoch: 24, Step: 2510, Loss: 0.665984\n",
      "Epoch: 24, Step: 2520, Loss: 1.92095\n",
      "Epoch: 24, Step: 2530, Loss: 0.657427\n",
      "Epoch: 24, Step: 2540, Loss: 0.694409\n",
      "Epoch: 24, Step: 2550, Loss: 0.0048238\n",
      "Epoch: 24, Step: 2560, Loss: 0.0124865\n",
      "Epoch: 24, Step: 2570, Loss: 0.670772\n",
      "Epoch: 24, Step: 2580, Loss: 1.50887\n",
      "Epoch: 24, Step: 2590, Loss: 0.107749\n",
      "Epoch: 24, Step: 2600, Loss: 0.108531\n",
      "Epoch: 24, Step: 2610, Loss: 0.00354859\n",
      "Epoch: 24, Step: 2620, Loss: 0.181956\n",
      "Epoch: 24, Step: 2630, Loss: 0.342108\n",
      "Epoch: 24, Step: 2640, Loss: 0.429738\n",
      "Epoch: 24, Step: 2650, Loss: 1.32987\n",
      "Epoch: 24, Step: 2660, Loss: 0.383127\n",
      "Epoch: 24, Step: 2670, Loss: 0.00290537\n",
      "Epoch: 24, Step: 2680, Loss: 0.0172849\n",
      "Epoch: 24, Step: 2690, Loss: 0.0328695\n",
      "Epoch: 24, Step: 2700, Loss: 0.0426286\n",
      "Epoch: 24, Step: 2710, Loss: 0.250152\n",
      "Epoch: 24, Step: 2720, Loss: 0.577254\n",
      "Epoch: 24, Step: 2730, Loss: 0.284816\n",
      "Epoch: 24, Step: 2740, Loss: 0.0928435\n",
      "Epoch: 24, Step: 2750, Loss: 0.0450249\n",
      "Epoch: 24, Step: 2760, Loss: 0.0110973\n",
      "Epoch: 24, Step: 2770, Loss: 0.105398\n",
      "Epoch: 24, Step: 2780, Loss: 0.764594\n",
      "Epoch: 24, Step: 2790, Loss: 1.45762\n",
      "Epoch: 24, Step: 2800, Loss: 0.275174\n",
      "Epoch: 24, Step: 2810, Loss: 0.125316\n",
      "Epoch: 24, Step: 2820, Loss: 0.00235872\n",
      "Epoch: 24, Step: 2830, Loss: 0.0060723\n",
      "Epoch: 24, Step: 2840, Loss: 0.002156\n",
      "Epoch: 24, Step: 2850, Loss: 0.0138623\n",
      "Epoch: 25, Step: 2500, Loss: 0.0373641\n",
      "Epoch: 25, Step: 2510, Loss: 0.0444022\n",
      "Epoch: 25, Step: 2520, Loss: 0.00560699\n",
      "Epoch: 25, Step: 2530, Loss: 0.121036\n",
      "Epoch: 25, Step: 2540, Loss: 0.38658\n",
      "Epoch: 25, Step: 2550, Loss: 0.111951\n",
      "Epoch: 25, Step: 2560, Loss: 0.192915\n",
      "Epoch: 25, Step: 2570, Loss: 0.0148236\n",
      "Epoch: 25, Step: 2580, Loss: 0.0154968\n",
      "Epoch: 25, Step: 2590, Loss: 0.015529\n",
      "Epoch: 25, Step: 2600, Loss: 0.0149394\n",
      "Epoch: 25, Step: 2610, Loss: 0.021582\n",
      "Epoch: 25, Step: 2620, Loss: 4.15254\n",
      "Epoch: 25, Step: 2630, Loss: 1.32575\n",
      "Epoch: 25, Step: 2640, Loss: 0.0113452\n",
      "Epoch: 25, Step: 2650, Loss: 0.00332626\n",
      "Epoch: 25, Step: 2660, Loss: 0.00448734\n",
      "Epoch: 25, Step: 2670, Loss: 0.00976086\n",
      "Epoch: 25, Step: 2680, Loss: 0.00448021\n",
      "Epoch: 25, Step: 2690, Loss: 0.00532632\n",
      "Epoch: 25, Step: 2700, Loss: 0.00235655\n",
      "Epoch: 25, Step: 2710, Loss: 0.00345547\n",
      "Epoch: 25, Step: 2720, Loss: 0.0040912\n",
      "Epoch: 25, Step: 2730, Loss: 0.00401576\n",
      "Epoch: 25, Step: 2740, Loss: 0.0117908\n",
      "Epoch: 25, Step: 2750, Loss: 0.0429328\n",
      "Epoch: 25, Step: 2760, Loss: 0.0311677\n",
      "Epoch: 25, Step: 2770, Loss: 0.00138352\n",
      "Epoch: 25, Step: 2780, Loss: 0.000842229\n",
      "Epoch: 25, Step: 2790, Loss: 0.00104807\n",
      "Epoch: 25, Step: 2800, Loss: 0.0014441\n",
      "Epoch: 25, Step: 2810, Loss: 0.000239185\n",
      "Epoch: 25, Step: 2820, Loss: 0.00196663\n",
      "Epoch: 25, Step: 2830, Loss: 0.000311637\n",
      "Epoch: 25, Step: 2840, Loss: 0.000476328\n",
      "Epoch: 25, Step: 2850, Loss: 0.00133141\n",
      "Epoch: 25, Step: 2860, Loss: 0.00106002\n",
      "Epoch: 25, Step: 2870, Loss: 0.000707824\n",
      "Epoch: 25, Step: 2880, Loss: 0.000466395\n",
      "Epoch: 25, Step: 2890, Loss: 0.000425971\n",
      "Epoch: 25, Step: 2900, Loss: 0.000495681\n",
      "Epoch: 25, Step: 2910, Loss: 0.00154556\n",
      "Epoch: 25, Step: 2920, Loss: 0.0481639\n",
      "Epoch: 25, Step: 2930, Loss: 0.0841738\n",
      "Epoch: 25, Step: 2940, Loss: 5.39078\n",
      "Epoch: 25, Step: 2950, Loss: 0.0407878\n",
      "Epoch: 26, Step: 2600, Loss: 0.00192899\n",
      "Epoch: 26, Step: 2610, Loss: 0.0263903\n",
      "Epoch: 26, Step: 2620, Loss: 0.0621309\n",
      "Epoch: 26, Step: 2630, Loss: 0.0617439\n",
      "Epoch: 26, Step: 2640, Loss: 0.00517104\n",
      "Epoch: 26, Step: 2650, Loss: 0.130651\n",
      "Epoch: 26, Step: 2660, Loss: 0.0606604\n",
      "Epoch: 26, Step: 2670, Loss: 0.00125306\n",
      "Epoch: 26, Step: 2680, Loss: 0.0539225\n",
      "Epoch: 26, Step: 2690, Loss: 0.0110903\n",
      "Epoch: 26, Step: 2700, Loss: 0.0171005\n",
      "Epoch: 26, Step: 2710, Loss: 0.0324625\n",
      "Epoch: 26, Step: 2720, Loss: 0.143575\n",
      "Epoch: 26, Step: 2730, Loss: 0.0431321\n",
      "Epoch: 26, Step: 2740, Loss: 0.126929\n",
      "Epoch: 26, Step: 2750, Loss: 0.0568689\n",
      "Epoch: 26, Step: 2760, Loss: 0.0353257\n",
      "Epoch: 26, Step: 2770, Loss: 0.0228459\n",
      "Epoch: 26, Step: 2780, Loss: 0.0180037\n",
      "Epoch: 26, Step: 2790, Loss: 0.0193547\n",
      "Epoch: 26, Step: 2800, Loss: 0.0197232\n",
      "Epoch: 26, Step: 2810, Loss: 0.0202363\n",
      "Epoch: 26, Step: 2820, Loss: 0.0214127\n",
      "Epoch: 26, Step: 2830, Loss: 0.0279266\n",
      "Epoch: 26, Step: 2840, Loss: 0.0321693\n",
      "Epoch: 26, Step: 2850, Loss: 0.0408228\n",
      "Epoch: 26, Step: 2860, Loss: 0.00937366\n",
      "Epoch: 26, Step: 2870, Loss: 0.0251321\n",
      "Epoch: 26, Step: 2880, Loss: 0.0947694\n",
      "Epoch: 26, Step: 2890, Loss: 0.0642619\n",
      "Epoch: 26, Step: 2900, Loss: 0.273808\n",
      "Epoch: 26, Step: 2910, Loss: 0.00949699\n",
      "Epoch: 26, Step: 2920, Loss: 0.102326\n",
      "Epoch: 26, Step: 2930, Loss: 0.102566\n",
      "Epoch: 26, Step: 2940, Loss: 0.366846\n",
      "Epoch: 26, Step: 2950, Loss: 0.0358053\n",
      "Epoch: 26, Step: 2960, Loss: 0.17848\n",
      "Epoch: 26, Step: 2970, Loss: 0.344733\n",
      "Epoch: 26, Step: 2980, Loss: 0.147864\n",
      "Epoch: 26, Step: 2990, Loss: 0.049656\n",
      "Epoch: 26, Step: 3000, Loss: 0.00812813\n",
      "Epoch: 26, Step: 3010, Loss: 0.0178254\n",
      "Epoch: 26, Step: 3020, Loss: 0.0371874\n",
      "Epoch: 26, Step: 3030, Loss: 0.452543\n",
      "Epoch: 26, Step: 3040, Loss: 0.525576\n",
      "Epoch: 26, Step: 3050, Loss: 0.491905\n",
      "Epoch: 27, Step: 2700, Loss: 0.952049\n",
      "Epoch: 27, Step: 2710, Loss: 0.21071\n",
      "Epoch: 27, Step: 2720, Loss: 0.613392\n",
      "Epoch: 27, Step: 2730, Loss: 0.234359\n",
      "Epoch: 27, Step: 2740, Loss: 0.00970584\n",
      "Epoch: 27, Step: 2750, Loss: 0.0240866\n",
      "Epoch: 27, Step: 2760, Loss: 0.0917\n",
      "Epoch: 27, Step: 2770, Loss: 0.167341\n",
      "Epoch: 27, Step: 2780, Loss: 0.964358\n",
      "Epoch: 27, Step: 2790, Loss: 0.275421\n",
      "Epoch: 27, Step: 2800, Loss: 2.25069\n",
      "Epoch: 27, Step: 2810, Loss: 0.515226\n",
      "Epoch: 27, Step: 2820, Loss: 0.936264\n",
      "Epoch: 27, Step: 2830, Loss: 0.0332512\n",
      "Epoch: 27, Step: 2840, Loss: 0.00809353\n",
      "Epoch: 27, Step: 2850, Loss: 0.371897\n",
      "Epoch: 27, Step: 2860, Loss: 1.45978\n",
      "Epoch: 27, Step: 2870, Loss: 0.377369\n",
      "Epoch: 27, Step: 2880, Loss: 0.135588\n",
      "Epoch: 27, Step: 2890, Loss: 0.00874105\n",
      "Epoch: 27, Step: 2900, Loss: 0.110849\n",
      "Epoch: 27, Step: 2910, Loss: 0.353049\n",
      "Epoch: 27, Step: 2920, Loss: 0.313013\n",
      "Epoch: 27, Step: 2930, Loss: 1.35393\n",
      "Epoch: 27, Step: 2940, Loss: 0.589232\n",
      "Epoch: 27, Step: 2950, Loss: 0.00595423\n",
      "Epoch: 27, Step: 2960, Loss: 0.0129057\n",
      "Epoch: 27, Step: 2970, Loss: 0.0259219\n",
      "Epoch: 27, Step: 2980, Loss: 0.0342058\n",
      "Epoch: 27, Step: 2990, Loss: 0.135648\n",
      "Epoch: 27, Step: 3000, Loss: 0.59674\n",
      "Epoch: 27, Step: 3010, Loss: 0.439603\n",
      "Epoch: 27, Step: 3020, Loss: 0.12761\n",
      "Epoch: 27, Step: 3030, Loss: 0.0443882\n",
      "Epoch: 27, Step: 3040, Loss: 0.0173344\n",
      "Epoch: 27, Step: 3050, Loss: 0.0648522\n",
      "Epoch: 27, Step: 3060, Loss: 0.515463\n",
      "Epoch: 27, Step: 3070, Loss: 1.42928\n",
      "Epoch: 27, Step: 3080, Loss: 0.543207\n",
      "Epoch: 27, Step: 3090, Loss: 0.11473\n",
      "Epoch: 27, Step: 3100, Loss: 0.0165764\n",
      "Epoch: 27, Step: 3110, Loss: 0.00561469\n",
      "Epoch: 27, Step: 3120, Loss: 0.000989301\n",
      "Epoch: 27, Step: 3130, Loss: 0.010176\n",
      "Epoch: 27, Step: 3140, Loss: 0.0293052\n",
      "Epoch: 27, Step: 3150, Loss: 0.0483801\n",
      "Epoch: 28, Step: 2800, Loss: 0.00261703\n",
      "Epoch: 28, Step: 2810, Loss: 0.0858938\n",
      "Epoch: 28, Step: 2820, Loss: 0.208012\n",
      "Epoch: 28, Step: 2830, Loss: 0.279746\n",
      "Epoch: 28, Step: 2840, Loss: 0.203332\n",
      "Epoch: 28, Step: 2850, Loss: 0.0100308\n",
      "Epoch: 28, Step: 2860, Loss: 0.00805584\n",
      "Epoch: 28, Step: 2870, Loss: 0.00825732\n",
      "Epoch: 28, Step: 2880, Loss: 0.00740194\n",
      "Epoch: 28, Step: 2890, Loss: 0.0134965\n",
      "Epoch: 28, Step: 2900, Loss: 2.55831\n",
      "Epoch: 28, Step: 2910, Loss: 3.00281\n",
      "Epoch: 28, Step: 2920, Loss: 0.0126031\n",
      "Epoch: 28, Step: 2930, Loss: 0.00435387\n",
      "Epoch: 28, Step: 2940, Loss: 0.00409621\n",
      "Epoch: 28, Step: 2950, Loss: 0.0116761\n",
      "Epoch: 28, Step: 2960, Loss: 0.00386901\n",
      "Epoch: 28, Step: 2970, Loss: 0.00620766\n",
      "Epoch: 28, Step: 2980, Loss: 0.00162992\n",
      "Epoch: 28, Step: 2990, Loss: 0.00217998\n",
      "Epoch: 28, Step: 3000, Loss: 0.00297244\n",
      "Epoch: 28, Step: 3010, Loss: 0.000712168\n",
      "Epoch: 28, Step: 3020, Loss: 0.00938279\n",
      "Epoch: 28, Step: 3030, Loss: 0.0441288\n",
      "Epoch: 28, Step: 3040, Loss: 0.0408738\n",
      "Epoch: 28, Step: 3050, Loss: 0.00205507\n",
      "Epoch: 28, Step: 3060, Loss: 0.000782093\n",
      "Epoch: 28, Step: 3070, Loss: 0.000675949\n",
      "Epoch: 28, Step: 3080, Loss: 0.00137624\n",
      "Epoch: 28, Step: 3090, Loss: 0.000333694\n",
      "Epoch: 28, Step: 3100, Loss: 0.0013993\n",
      "Epoch: 28, Step: 3110, Loss: 0.000571063\n",
      "Epoch: 28, Step: 3120, Loss: 0.000433831\n",
      "Epoch: 28, Step: 3130, Loss: 0.00124134\n",
      "Epoch: 28, Step: 3140, Loss: 0.00107808\n",
      "Epoch: 28, Step: 3150, Loss: 0.00202433\n",
      "Epoch: 28, Step: 3160, Loss: 0.00126533\n",
      "Epoch: 28, Step: 3170, Loss: 0.0020669\n",
      "Epoch: 28, Step: 3180, Loss: 0.0024037\n",
      "Epoch: 28, Step: 3190, Loss: 0.00284898\n",
      "Epoch: 28, Step: 3200, Loss: 0.0409839\n",
      "Epoch: 28, Step: 3210, Loss: 0.0310731\n",
      "Epoch: 28, Step: 3220, Loss: 4.79666\n",
      "Epoch: 28, Step: 3230, Loss: 0.784123\n",
      "Epoch: 28, Step: 3240, Loss: 0.00222112\n",
      "Epoch: 28, Step: 3250, Loss: 0.0172196\n",
      "Epoch: 29, Step: 2900, Loss: 0.0606206\n",
      "Epoch: 29, Step: 2910, Loss: 0.0738105\n",
      "Epoch: 29, Step: 2920, Loss: 0.012082\n",
      "Epoch: 29, Step: 2930, Loss: 0.0885343\n",
      "Epoch: 29, Step: 2940, Loss: 0.0776773\n",
      "Epoch: 29, Step: 2950, Loss: 0.00238931\n",
      "Epoch: 29, Step: 2960, Loss: 0.047134\n",
      "Epoch: 29, Step: 2970, Loss: 0.0130425\n",
      "Epoch: 29, Step: 2980, Loss: 0.0148652\n",
      "Epoch: 29, Step: 2990, Loss: 0.00538508\n",
      "Epoch: 29, Step: 3000, Loss: 0.154629\n",
      "Epoch: 29, Step: 3010, Loss: 0.0423541\n",
      "Epoch: 29, Step: 3020, Loss: 0.12745\n",
      "Epoch: 29, Step: 3030, Loss: 0.0727394\n",
      "Epoch: 29, Step: 3040, Loss: 0.0372171\n",
      "Epoch: 29, Step: 3050, Loss: 0.0228985\n",
      "Epoch: 29, Step: 3060, Loss: 0.0241091\n",
      "Epoch: 29, Step: 3070, Loss: 0.0226413\n",
      "Epoch: 29, Step: 3080, Loss: 0.0229869\n",
      "Epoch: 29, Step: 3090, Loss: 0.0221935\n",
      "Epoch: 29, Step: 3100, Loss: 0.0180779\n",
      "Epoch: 29, Step: 3110, Loss: 0.0153499\n",
      "Epoch: 29, Step: 3120, Loss: 0.0303792\n",
      "Epoch: 29, Step: 3130, Loss: 0.0255999\n",
      "Epoch: 29, Step: 3140, Loss: 0.0128588\n",
      "Epoch: 29, Step: 3150, Loss: 0.0130605\n",
      "Epoch: 29, Step: 3160, Loss: 0.062095\n",
      "Epoch: 29, Step: 3170, Loss: 0.0353495\n",
      "Epoch: 29, Step: 3180, Loss: 0.331929\n",
      "Epoch: 29, Step: 3190, Loss: 0.0113773\n",
      "Epoch: 29, Step: 3200, Loss: 0.0761934\n",
      "Epoch: 29, Step: 3210, Loss: 0.0887019\n",
      "Epoch: 29, Step: 3220, Loss: 0.394221\n",
      "Epoch: 29, Step: 3230, Loss: 0.0595832\n",
      "Epoch: 29, Step: 3240, Loss: 0.108754\n",
      "Epoch: 29, Step: 3250, Loss: 0.276723\n",
      "Epoch: 29, Step: 3260, Loss: 0.228301\n",
      "Epoch: 29, Step: 3270, Loss: 0.0240523\n",
      "Epoch: 29, Step: 3280, Loss: 0.0295599\n",
      "Epoch: 29, Step: 3290, Loss: 0.0112427\n",
      "Epoch: 29, Step: 3300, Loss: 0.0534368\n",
      "Epoch: 29, Step: 3310, Loss: 0.148452\n",
      "Epoch: 29, Step: 3320, Loss: 0.800224\n",
      "Epoch: 29, Step: 3330, Loss: 0.269286\n",
      "Epoch: 29, Step: 3340, Loss: 0.968776\n",
      "Epoch: 29, Step: 3350, Loss: 0.450521\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.protobuf import saver_pb2\n",
    "\n",
    "# import driving_data\n",
    "# import model\n",
    "\n",
    "LOGDIR = './save'\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_, y))) + tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) * L2NormConst\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "# merge all summaries into a single op\n",
    "merged_summary_op =  tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver(write_version = saver_pb2.SaverDef.V1)\n",
    "\n",
    "# op to write logs to Tensorboard\n",
    "logs_path = './logs'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# train over the dataset about 30 times\n",
    "for epoch in range(epochs):\n",
    "  for i in range(int(num_images/batch_size)):\n",
    "    xs, ys = LoadTrainBatch(batch_size)\n",
    "    train_step.run(feed_dict={x: xs, y_: ys, keep_prob: 0.5})\n",
    "    if i % 10 == 0:\n",
    "      xs, ys = LoadValBatch(batch_size)\n",
    "      loss_value = loss.eval(feed_dict={x:xs, y_: ys, keep_prob: 1.0})\n",
    "      print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + i, loss_value))\n",
    "\n",
    "    # write logs at every iteration\n",
    "    summary = merged_summary_op.eval(feed_dict={x:xs, y_: ys, keep_prob: 1.0})\n",
    "    summary_writer.add_summary(summary, epoch * num_images/batch_size + i)\n",
    "\n",
    "#     if i % batch_size == 0:\n",
    "#       if not os.path.exists(LOGDIR):\n",
    "#         os.makedirs(LOGDIR)\n",
    "#       checkpoint_path = os.path.join(LOGDIR, \"model.ckpt\")\n",
    "#       filename = saver.save(sess, checkpoint_path)\n",
    "#   print(\"Model saved in file: %s\" % filename)\n",
    "\n",
    "# print(\"Run the command line:\\n\" \\\n",
    "#           \"--> tensorboard --logdir=./logs \" \\\n",
    "#           \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Self_Driving_Car.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
