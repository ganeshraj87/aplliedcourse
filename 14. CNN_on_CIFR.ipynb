{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "#from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten,GlobalAveragePooling2D,AveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "#dropout_rate = 0.2\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mB7o3zu1g6eT"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.utils import resample\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) \n",
    "# y_cv = tf.keras.utils.to_categorical(y_cv, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bmUkPEdWSgMu",
    "outputId": "716d8251-27db-490e-ea9e-d51090ddb34d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAEXUQs9xmiX"
   },
   "outputs": [],
   "source": [
    "# X_cv=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TWmTvTi7SgMw",
    "outputId": "ac628af1-df32-4e50-b85e-683c4a1dc462"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHTEuJpOexes"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "#https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    global weight_decay\n",
    "    temp = input\n",
    "    input_conv1 = num_filter*4\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization(beta_regularizer=regularizers.l2(weight_decay))(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(input_conv1),1, padding='same',kernel_initializer=\"he_normal\",use_bias=False ,kernel_regularizer=regularizers.l2(weight_decay))(relu)\n",
    "\n",
    "        BatchNorm = layers.BatchNormalization(beta_regularizer=regularizers.l2(weight_decay))(Conv2D_3_3)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_4 = layers.Conv2D(int(num_filter),3, padding='same',kernel_initializer=\"he_normal\",use_bias=False ,kernel_regularizer=regularizers.l2(weight_decay))(relu)\n",
    "\n",
    "        concat = layers.Concatenate()([temp,Conv2D_3_4])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "\n",
    "## transition Blosck\n",
    "#https://arthurdouillard.com/post/densenet/\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "  \n",
    "    global compression\n",
    "    global weight_decay\n",
    "    channel = input.shape.as_list()[-1]\n",
    "    input_conv = channel*compression\n",
    "    BatchNorm = layers.BatchNormalization(beta_regularizer=regularizers.l2(weight_decay))(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(input_conv), 1,use_bias=False ,padding='same',kernel_regularizer=regularizers.l2(weight_decay))(relu)\n",
    "    Conv2D_BottleNeck = layers.Activation('relu')(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "\n",
    "# #output layer\n",
    "def output_layer(input):\n",
    "    input_conv = num_filter*1\n",
    "    BatchNorm = BatchNormalization(beta_regularizer=regularizers.l2(weight_decay))(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(8,8))(relu)\n",
    "    #output = layers.Conv2D(10,1,use_bias=False ,padding='same',kernel_regularizer=regularizers.l2(weight_decay))(AvgPooling)\n",
    "    #flat = Flatten()(AvgPooling)\n",
    "    output = layers.Conv2D(10,1,use_bias=False ,padding='same',kernel_regularizer=regularizers.l2(weight_decay))(AvgPooling)\n",
    "    output = Activation('softmax')(output)\n",
    "    #output = AveragePooling2D(pool_size=(1,1))(output)\n",
    "    output = Flatten()(output)\n",
    "    #output  = BatchNormalization(beta_regularizer=regularizers.l2(weight_decay))(output)\n",
    "   # output = Flatten()(output)\n",
    "   # output = layers.Conv2D(num_classes,2, activation='softmax',kernel_regularizer=regularizers.l2(weight_decay))(flat)\n",
    "   # output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers \n",
    "num_filter = 12\n",
    "dropout_rate = 0.2\n",
    "l = 16\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(2*num_filter, 3, use_bias=False ,padding='same',bias_initializer='zeros',kernel_regularizer=regularizers.l2(weight_decay))(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "\n",
    "output = output_layer(Third_Block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "c392a131-81bd-489a-d89d-570aaa7be629",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 32, 32, 24)   648         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 32, 32, 24)   96          conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 32, 32, 24)   0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 32, 32, 48)   1152        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 32, 32, 48)   192         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 32, 32, 48)   0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 32, 32, 12)   5184        activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 32, 32, 36)   0           conv2d_100[0][0]                 \n",
      "                                                                 conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 32, 32, 36)   144         concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 32, 32, 36)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 32, 32, 48)   1728        activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 32, 32, 48)   192         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 32, 32, 48)   0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 32, 32, 12)   5184        activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 32, 32, 48)   0           concatenate_48[0][0]             \n",
      "                                                                 conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 32, 32, 48)   192         concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 32, 32, 48)   0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 32, 32, 48)   2304        activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 32, 32, 48)   192         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 32, 32, 48)   0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 32, 32, 12)   5184        activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 32, 32, 60)   0           concatenate_49[0][0]             \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 32, 32, 60)   240         concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 32, 32, 60)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 32, 32, 48)   2880        activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 32, 32, 48)   192         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 32, 32, 48)   0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 32, 32, 12)   5184        activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 32, 32, 72)   0           concatenate_50[0][0]             \n",
      "                                                                 conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 32, 32, 72)   288         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 32, 32, 72)   0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 32, 32, 48)   3456        activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 32, 32, 48)   192         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 32, 32, 48)   0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 32, 32, 12)   5184        activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 32, 32, 84)   0           concatenate_51[0][0]             \n",
      "                                                                 conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 32, 32, 84)   336         concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 32, 32, 84)   0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 32, 32, 48)   4032        activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 32, 32, 48)   192         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 32, 32, 48)   0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 32, 32, 12)   5184        activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 32, 32, 96)   0           concatenate_52[0][0]             \n",
      "                                                                 conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 32, 32, 96)   384         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 32, 32, 96)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 32, 32, 48)   4608        activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 32, 32, 48)   192         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 32, 32, 48)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 32, 32, 12)   5184        activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 32, 32, 108)  0           concatenate_53[0][0]             \n",
      "                                                                 conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 32, 32, 108)  432         concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 32, 32, 108)  0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 32, 32, 48)   5184        activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 32, 32, 48)   192         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 32, 32, 48)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 32, 32, 12)   5184        activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 32, 32, 120)  0           concatenate_54[0][0]             \n",
      "                                                                 conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 32, 32, 120)  480         concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 32, 32, 120)  0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 32, 32, 48)   5760        activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 32, 32, 48)   192         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 32, 32, 48)   0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 32, 32, 12)   5184        activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 32, 32, 132)  0           concatenate_55[0][0]             \n",
      "                                                                 conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 32, 32, 132)  528         concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 32, 32, 132)  0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 32, 32, 48)   6336        activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 32, 32, 48)   192         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 32, 32, 48)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 32, 32, 12)   5184        activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 32, 32, 144)  0           concatenate_56[0][0]             \n",
      "                                                                 conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 32, 32, 144)  576         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 32, 32, 144)  0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 32, 32, 48)   6912        activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 32, 32, 48)   192         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 32, 32, 48)   0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 32, 32, 12)   5184        activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 32, 32, 156)  0           concatenate_57[0][0]             \n",
      "                                                                 conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 32, 32, 156)  624         concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 32, 32, 156)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 32, 32, 48)   7488        activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 32, 32, 48)   192         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 32, 32, 48)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 32, 32, 12)   5184        activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 32, 32, 168)  0           concatenate_58[0][0]             \n",
      "                                                                 conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 32, 32, 168)  672         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 32, 32, 168)  0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 32, 32, 48)   8064        activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 32, 32, 48)   192         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 32, 32, 48)   0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 32, 32, 12)   5184        activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 32, 32, 180)  0           concatenate_59[0][0]             \n",
      "                                                                 conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 32, 32, 180)  720         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 32, 32, 180)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 32, 32, 48)   8640        activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 32, 32, 48)   192         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 32, 32, 48)   0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 32, 32, 12)   5184        activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 32, 32, 192)  0           concatenate_60[0][0]             \n",
      "                                                                 conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 32, 32, 192)  768         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 32, 32, 192)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 32, 32, 48)   9216        activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 32, 32, 48)   192         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 32, 32, 48)   0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 32, 32, 12)   5184        activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 32, 32, 204)  0           concatenate_61[0][0]             \n",
      "                                                                 conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 32, 32, 204)  816         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 32, 32, 204)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 32, 32, 48)   9792        activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 32, 32, 48)   192         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 32, 32, 48)   0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 32, 32, 12)   5184        activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 32, 32, 216)  0           concatenate_62[0][0]             \n",
      "                                                                 conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 32, 32, 216)  864         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 32, 32, 216)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 32, 32, 108)  23328       activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 32, 32, 108)  0           conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 16, 16, 108)  0           activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 16, 16, 108)  432         average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 16, 16, 108)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 16, 16, 48)   5184        activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 16, 16, 48)   192         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 16, 16, 48)   0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 16, 16, 12)   5184        activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 16, 16, 120)  0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 16, 16, 120)  480         concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 16, 16, 120)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 16, 16, 48)   5760        activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 16, 16, 48)   192         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 16, 16, 48)   0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 16, 16, 12)   5184        activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 16, 16, 132)  0           concatenate_64[0][0]             \n",
      "                                                                 conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 16, 16, 132)  528         concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 16, 16, 132)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 16, 16, 48)   6336        activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 16, 16, 48)   192         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 16, 16, 48)   0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 16, 16, 12)   5184        activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 16, 16, 144)  0           concatenate_65[0][0]             \n",
      "                                                                 conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 16, 16, 144)  576         concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 16, 16, 144)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 16, 16, 48)   6912        activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 16, 16, 48)   192         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 16, 16, 48)   0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 16, 16, 12)   5184        activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 16, 16, 156)  0           concatenate_66[0][0]             \n",
      "                                                                 conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 16, 16, 156)  624         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 16, 16, 156)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 16, 16, 48)   7488        activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 16, 16, 48)   192         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 16, 16, 48)   0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 16, 16, 12)   5184        activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 16, 16, 168)  0           concatenate_67[0][0]             \n",
      "                                                                 conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 168)  672         concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 16, 16, 168)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 16, 16, 48)   8064        activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 16, 16, 48)   192         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 16, 16, 48)   0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 16, 16, 12)   5184        activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 16, 16, 180)  0           concatenate_68[0][0]             \n",
      "                                                                 conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 16, 16, 180)  720         concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 16, 16, 180)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 16, 16, 48)   8640        activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 16, 16, 48)   192         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 16, 16, 48)   0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 16, 16, 12)   5184        activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 16, 16, 192)  0           concatenate_69[0][0]             \n",
      "                                                                 conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 16, 16, 192)  768         concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 16, 16, 192)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 16, 16, 48)   9216        activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 16, 16, 48)   192         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 16, 16, 48)   0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 16, 16, 12)   5184        activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 16, 16, 204)  0           concatenate_70[0][0]             \n",
      "                                                                 conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 16, 16, 204)  816         concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 16, 16, 204)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 16, 48)   9792        activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 16, 16, 48)   192         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 16, 16, 48)   0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 16, 12)   5184        activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 16, 16, 216)  0           concatenate_71[0][0]             \n",
      "                                                                 conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 16, 16, 216)  864         concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 16, 16, 216)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 16, 16, 48)   10368       activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 16, 16, 48)   192         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 16, 16, 48)   0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 16, 16, 12)   5184        activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 16, 16, 228)  0           concatenate_72[0][0]             \n",
      "                                                                 conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 16, 16, 228)  912         concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 16, 16, 228)  0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 16, 16, 48)   10944       activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 16, 16, 48)   192         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 16, 16, 48)   0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 16, 16, 12)   5184        activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 16, 16, 240)  0           concatenate_73[0][0]             \n",
      "                                                                 conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 16, 16, 240)  960         concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 16, 16, 240)  0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 16, 16, 48)   11520       activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 16, 16, 48)   192         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 16, 16, 48)   0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 16, 16, 12)   5184        activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 16, 16, 252)  0           concatenate_74[0][0]             \n",
      "                                                                 conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 16, 16, 252)  1008        concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 16, 16, 252)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 16, 16, 48)   12096       activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 16, 16, 48)   192         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 16, 16, 48)   0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 16, 16, 12)   5184        activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 16, 16, 264)  0           concatenate_75[0][0]             \n",
      "                                                                 conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 16, 16, 264)  1056        concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 16, 16, 264)  0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 16, 16, 48)   12672       activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 16, 16, 48)   192         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 16, 16, 48)   0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 16, 16, 12)   5184        activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 16, 16, 276)  0           concatenate_76[0][0]             \n",
      "                                                                 conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 16, 16, 276)  1104        concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 16, 16, 276)  0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 16, 16, 48)   13248       activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 16, 16, 48)   192         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 16, 16, 48)   0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 16, 16, 12)   5184        activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 16, 16, 288)  0           concatenate_77[0][0]             \n",
      "                                                                 conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 16, 16, 288)  1152        concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 16, 16, 288)  0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 16, 16, 48)   13824       activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 16, 16, 48)   192         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 16, 16, 48)   0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 16, 16, 12)   5184        activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 16, 16, 300)  0           concatenate_78[0][0]             \n",
      "                                                                 conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 16, 16, 300)  1200        concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 16, 16, 300)  0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 16, 16, 150)  45000       activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 16, 16, 150)  0           conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 8, 8, 150)    0           activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 8, 8, 150)    600         average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 8, 8, 150)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 8, 8, 48)     7200        activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 8, 8, 48)     192         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 8, 8, 48)     0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 8, 8, 12)     5184        activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 8, 8, 162)    0           average_pooling2d_4[0][0]        \n",
      "                                                                 conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 8, 8, 162)    648         concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 8, 8, 162)    0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 8, 8, 48)     7776        activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 8, 8, 48)     192         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 8, 8, 48)     0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 8, 8, 12)     5184        activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 8, 8, 174)    0           concatenate_80[0][0]             \n",
      "                                                                 conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 8, 8, 174)    696         concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 8, 8, 174)    0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 8, 8, 48)     8352        activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 8, 8, 48)     192         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 8, 8, 48)     0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 8, 8, 12)     5184        activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 8, 8, 186)    0           concatenate_81[0][0]             \n",
      "                                                                 conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 8, 8, 186)    744         concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 8, 8, 186)    0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 8, 8, 48)     8928        activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 8, 8, 48)     192         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 8, 8, 48)     0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 8, 8, 12)     5184        activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 8, 8, 198)    0           concatenate_82[0][0]             \n",
      "                                                                 conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 8, 8, 198)    792         concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 8, 8, 198)    0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 8, 8, 48)     9504        activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 8, 8, 48)     192         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 8, 8, 48)     0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 8, 8, 12)     5184        activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 8, 8, 210)    0           concatenate_83[0][0]             \n",
      "                                                                 conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 8, 8, 210)    840         concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 8, 8, 210)    0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 8, 8, 48)     10080       activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 8, 8, 48)     192         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 8, 8, 48)     0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 8, 8, 12)     5184        activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 8, 8, 222)    0           concatenate_84[0][0]             \n",
      "                                                                 conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 8, 8, 222)    888         concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 8, 8, 222)    0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 8, 8, 48)     10656       activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 8, 8, 48)     192         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 8, 8, 48)     0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 8, 8, 12)     5184        activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 8, 8, 234)    0           concatenate_85[0][0]             \n",
      "                                                                 conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 8, 8, 234)    936         concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 8, 8, 234)    0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 8, 8, 48)     11232       activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 8, 8, 48)     192         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 8, 8, 48)     0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 8, 8, 12)     5184        activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 8, 8, 246)    0           concatenate_86[0][0]             \n",
      "                                                                 conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 8, 8, 246)    984         concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 8, 8, 246)    0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 8, 8, 48)     11808       activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 8, 8, 48)     192         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 8, 8, 48)     0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 8, 8, 12)     5184        activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 8, 8, 258)    0           concatenate_87[0][0]             \n",
      "                                                                 conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 8, 8, 258)    1032        concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 8, 8, 258)    0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 8, 8, 48)     12384       activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 8, 8, 48)     192         conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 8, 8, 48)     0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 8, 8, 12)     5184        activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 8, 8, 270)    0           concatenate_88[0][0]             \n",
      "                                                                 conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 8, 8, 270)    1080        concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 8, 8, 270)    0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 8, 8, 48)     12960       activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 8, 8, 48)     192         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 8, 8, 48)     0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 8, 8, 12)     5184        activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_90 (Concatenate)    (None, 8, 8, 282)    0           concatenate_89[0][0]             \n",
      "                                                                 conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 8, 8, 282)    1128        concatenate_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 8, 8, 282)    0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 8, 8, 48)     13536       activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 8, 8, 48)     192         conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 8, 8, 48)     0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 8, 8, 12)     5184        activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_91 (Concatenate)    (None, 8, 8, 294)    0           concatenate_90[0][0]             \n",
      "                                                                 conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 8, 8, 294)    1176        concatenate_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 8, 8, 294)    0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 8, 8, 48)     14112       activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 8, 8, 48)     192         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 8, 8, 48)     0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 8, 8, 12)     5184        activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 8, 8, 306)    0           concatenate_91[0][0]             \n",
      "                                                                 conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 8, 8, 306)    1224        concatenate_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 8, 8, 306)    0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 8, 8, 48)     14688       activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 8, 8, 48)     192         conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 8, 8, 48)     0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 8, 8, 12)     5184        activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 8, 8, 318)    0           concatenate_92[0][0]             \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 8, 8, 318)    1272        concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 8, 8, 318)    0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 8, 8, 48)     15264       activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 8, 8, 48)     192         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 8, 8, 48)     0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 8, 8, 12)     5184        activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 8, 8, 330)    0           concatenate_93[0][0]             \n",
      "                                                                 conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 8, 8, 330)    1320        concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 8, 8, 330)    0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 8, 8, 48)     15840       activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 8, 8, 48)     192         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 8, 8, 48)     0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 8, 8, 12)     5184        activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 8, 8, 342)    0           concatenate_94[0][0]             \n",
      "                                                                 conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 8, 8, 342)    1368        concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 8, 8, 342)    0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 342)    0           activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 1, 1, 10)     3420        average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 1, 1, 10)     0           conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           activation_203[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 793,140\n",
      "Trainable params: 769,152\n",
      "Non-trainable params: 23,988\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Model(inputs=[input], outputs=[output])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACN0TrdYgiPu"
   },
   "outputs": [],
   "source": [
    "#https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    zoom_range=0.32,\n",
    "    rotation_range=18,\n",
    " #   height_shift_range=0.1,\n",
    "#     width_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "   # vertical_flip=True,\n",
    "    rescale=1./255,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "cv_datagen = ImageDataGenerator(\n",
    "  #  zoom_range=0.32,\n",
    "  #  rotation_range=18,\n",
    " #   height_shift_range=0.1,\n",
    "#     width_shift_range=0.1,\n",
    "  #  horizontal_flip=True,\n",
    "   # vertical_flip=True,\n",
    "    rescale=1./255,\n",
    "#    fill_mode='nearest'\n",
    "    )\n",
    "test_datagen = ImageDataGenerator(\n",
    "   # zoom_range=0.32,\n",
    " #   rotation_range=18,\n",
    " #   height_shift_range=0.1,\n",
    "#     width_shift_range=0.1,\n",
    "  #  horizontal_flip=True,\n",
    "   # vertical_flip=True,\n",
    "    rescale=1./255,\n",
    "   # fill_mode='nearest'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsRm-hhggj6o"
   },
   "outputs": [],
   "source": [
    "train_datagen.fit(X_train)\n",
    "# cv_datagen.fit(X_cv)\n",
    "test_datagen.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3MLMnCOhhxI"
   },
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "val_batch_size = 64\n",
    "steps_per_epoch= len(y_train)//batch_size\n",
    "validation_steps = len(y_test)//val_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqYfm5qUiySo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = \"/content/drive/My Drive/saved-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txjx2C-i8urr"
   },
   "outputs": [],
   "source": [
    "model1 = Model(inputs=[input], outputs=[output])\n",
    "\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Nadam(lr=0.001,beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gqTP7cb4dFTa",
    "outputId": "1e974ad2-4f8d-40ca-c2b5-79ae0c1d3281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.7130 - acc: 0.5727Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:28 - loss: 2.0870 - acc: 0.4856\n",
      "Epoch 00001: val_acc improved from -inf to 0.48565, saving model to /content/drive/My Drive/saved-model-01-0.49.hdf5\n",
      "781/781 [==============================] - 649s 831ms/step - loss: 1.7126 - acc: 0.5728 - val_loss: 2.0870 - val_acc: 0.4856\n",
      "Epoch 2/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.1634 - acc: 0.7365Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 1.0995 - acc: 0.7532\n",
      "Epoch 00002: val_acc improved from 0.48565 to 0.75322, saving model to /content/drive/My Drive/saved-model-02-0.75.hdf5\n",
      "781/781 [==============================] - 388s 496ms/step - loss: 1.1632 - acc: 0.7365 - val_loss: 1.0995 - val_acc: 0.7532\n",
      "Epoch 3/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9561 - acc: 0.7911Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 1.3010 - acc: 0.6930\n",
      "Epoch 00003: val_acc did not improve from 0.75322\n",
      "781/781 [==============================] - 385s 492ms/step - loss: 0.9560 - acc: 0.7912 - val_loss: 1.3010 - val_acc: 0.6930\n",
      "Epoch 4/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8359 - acc: 0.8184Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.9876 - acc: 0.7829\n",
      "Epoch 00004: val_acc improved from 0.75322 to 0.78294, saving model to /content/drive/My Drive/saved-model-04-0.78.hdf5\n",
      "781/781 [==============================] - 387s 495ms/step - loss: 0.8358 - acc: 0.8185 - val_loss: 0.9876 - val_acc: 0.7829\n",
      "Epoch 5/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7590 - acc: 0.8383Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.8788 - acc: 0.8059\n",
      "Epoch 00005: val_acc improved from 0.78294 to 0.80590, saving model to /content/drive/My Drive/saved-model-05-0.81.hdf5\n",
      "781/781 [==============================] - 384s 492ms/step - loss: 0.7591 - acc: 0.8383 - val_loss: 0.8788 - val_acc: 0.8059\n",
      "Epoch 6/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7031 - acc: 0.8508Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.7847 - acc: 0.8305\n",
      "Epoch 00006: val_acc improved from 0.80590 to 0.83048, saving model to /content/drive/My Drive/saved-model-06-0.83.hdf5\n",
      "781/781 [==============================] - 386s 494ms/step - loss: 0.7029 - acc: 0.8508 - val_loss: 0.7847 - val_acc: 0.8305\n",
      "Epoch 7/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.6629 - acc: 0.8603Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.8340 - acc: 0.8171\n",
      "Epoch 00007: val_acc did not improve from 0.83048\n",
      "781/781 [==============================] - 384s 491ms/step - loss: 0.6630 - acc: 0.8602 - val_loss: 0.8340 - val_acc: 0.8171\n",
      "Epoch 8/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.6291 - acc: 0.8687Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.9281 - acc: 0.7881\n",
      "Epoch 00008: val_acc did not improve from 0.83048\n",
      "781/781 [==============================] - 384s 492ms/step - loss: 0.6290 - acc: 0.8688 - val_loss: 0.9281 - val_acc: 0.7881\n",
      "Epoch 9/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.6050 - acc: 0.8747Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:09 - loss: 0.7100 - acc: 0.8417\n",
      "Epoch 00009: val_acc improved from 0.83048 to 0.84166, saving model to /content/drive/My Drive/saved-model-09-0.84.hdf5\n",
      "781/781 [==============================] - 386s 494ms/step - loss: 0.6048 - acc: 0.8748 - val_loss: 0.7100 - val_acc: 0.8417\n",
      "Epoch 10/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5819 - acc: 0.8792Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.6858 - acc: 0.8535\n",
      "Epoch 00010: val_acc improved from 0.84166 to 0.85355, saving model to /content/drive/My Drive/saved-model-10-0.85.hdf5\n",
      "781/781 [==============================] - 387s 495ms/step - loss: 0.5817 - acc: 0.8792 - val_loss: 0.6858 - val_acc: 0.8535\n",
      "Epoch 11/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5648 - acc: 0.8844Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.8029 - acc: 0.8201\n",
      "Epoch 00011: val_acc did not improve from 0.85355\n",
      "781/781 [==============================] - 386s 495ms/step - loss: 0.5648 - acc: 0.8844 - val_loss: 0.8029 - val_acc: 0.8201\n",
      "Epoch 12/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.8873Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:09 - loss: 0.8914 - acc: 0.7908\n",
      "Epoch 00012: val_acc did not improve from 0.85355\n",
      "781/781 [==============================] - 386s 494ms/step - loss: 0.5501 - acc: 0.8873 - val_loss: 0.8914 - val_acc: 0.7908\n",
      "Epoch 13/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8934Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.6179 - acc: 0.8747\n",
      "Epoch 00013: val_acc improved from 0.85355 to 0.87475, saving model to /content/drive/My Drive/saved-model-13-0.87.hdf5\n",
      "781/781 [==============================] - 387s 496ms/step - loss: 0.5334 - acc: 0.8935 - val_loss: 0.6179 - val_acc: 0.8747\n",
      "Epoch 14/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.8943Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.6711 - acc: 0.8547\n",
      "Epoch 00014: val_acc did not improve from 0.87475\n",
      "781/781 [==============================] - 387s 495ms/step - loss: 0.5253 - acc: 0.8944 - val_loss: 0.6711 - val_acc: 0.8547\n",
      "Epoch 15/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8988Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.5995 - acc: 0.8751\n",
      "Epoch 00015: val_acc improved from 0.87475 to 0.87510, saving model to /content/drive/My Drive/saved-model-15-0.88.hdf5\n",
      "781/781 [==============================] - 387s 496ms/step - loss: 0.5143 - acc: 0.8989 - val_loss: 0.5995 - val_acc: 0.8751\n",
      "Epoch 16/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.9011Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.7878 - acc: 0.8273\n",
      "Epoch 00016: val_acc did not improve from 0.87510\n",
      "781/781 [==============================] - 386s 494ms/step - loss: 0.5038 - acc: 0.9010 - val_loss: 0.7878 - val_acc: 0.8273\n",
      "Epoch 17/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.9039Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:09 - loss: 0.7624 - acc: 0.8328\n",
      "Epoch 00017: val_acc did not improve from 0.87510\n",
      "781/781 [==============================] - 388s 497ms/step - loss: 0.4949 - acc: 0.9039 - val_loss: 0.7624 - val_acc: 0.8328\n",
      "Epoch 18/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4884 - acc: 0.9058Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:09 - loss: 0.7809 - acc: 0.8160\n",
      "Epoch 00018: val_acc did not improve from 0.87510\n",
      "781/781 [==============================] - 387s 495ms/step - loss: 0.4884 - acc: 0.9058 - val_loss: 0.7809 - val_acc: 0.8160\n",
      "Epoch 19/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.9077Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:09 - loss: 0.6219 - acc: 0.8725\n",
      "Epoch 00019: val_acc did not improve from 0.87510\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4816 - acc: 0.9077 - val_loss: 0.6219 - val_acc: 0.8725\n",
      "Epoch 20/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4747 - acc: 0.9093Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.5303 - acc: 0.8962\n",
      "Epoch 00020: val_acc improved from 0.87510 to 0.89615, saving model to /content/drive/My Drive/saved-model-20-0.90.hdf5\n",
      "781/781 [==============================] - 387s 495ms/step - loss: 0.4748 - acc: 0.9092 - val_loss: 0.5303 - val_acc: 0.8962\n",
      "Epoch 21/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4734 - acc: 0.9088Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.7677 - acc: 0.8275\n",
      "Epoch 00021: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4735 - acc: 0.9088 - val_loss: 0.7677 - val_acc: 0.8275\n",
      "Epoch 22/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.9134Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.7488 - acc: 0.8442\n",
      "Epoch 00022: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4628 - acc: 0.9134 - val_loss: 0.7488 - val_acc: 0.8442\n",
      "Epoch 23/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4585 - acc: 0.9142Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.7097 - acc: 0.8467\n",
      "Epoch 00023: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4587 - acc: 0.9141 - val_loss: 0.7097 - val_acc: 0.8467\n",
      "Epoch 24/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4559 - acc: 0.9155Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.8292 - acc: 0.8206\n",
      "Epoch 00024: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4560 - acc: 0.9155 - val_loss: 0.8292 - val_acc: 0.8206\n",
      "Epoch 25/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.9174Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.6769 - acc: 0.8565\n",
      "Epoch 00025: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4493 - acc: 0.9174 - val_loss: 0.6769 - val_acc: 0.8565\n",
      "Epoch 26/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.9187Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.5559 - acc: 0.8886\n",
      "Epoch 00026: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4435 - acc: 0.9187 - val_loss: 0.5559 - val_acc: 0.8886\n",
      "Epoch 27/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4390 - acc: 0.9197Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.7819 - acc: 0.8301\n",
      "Epoch 00027: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 382s 490ms/step - loss: 0.4389 - acc: 0.9197 - val_loss: 0.7819 - val_acc: 0.8301\n",
      "Epoch 28/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.9193Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.5844 - acc: 0.8800\n",
      "Epoch 00028: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 384s 492ms/step - loss: 0.4375 - acc: 0.9193 - val_loss: 0.5844 - val_acc: 0.8800\n",
      "Epoch 29/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.9222Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.6743 - acc: 0.8695\n",
      "Epoch 00029: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4307 - acc: 0.9222 - val_loss: 0.6743 - val_acc: 0.8695\n",
      "Epoch 30/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.9216Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.6025 - acc: 0.8756\n",
      "Epoch 00030: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4287 - acc: 0.9216 - val_loss: 0.6025 - val_acc: 0.8756\n",
      "Epoch 31/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.9236Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.5552 - acc: 0.8879\n",
      "Epoch 00031: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 384s 492ms/step - loss: 0.4248 - acc: 0.9235 - val_loss: 0.5552 - val_acc: 0.8879\n",
      "Epoch 32/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.9238Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.7309 - acc: 0.8468\n",
      "Epoch 00032: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 387s 495ms/step - loss: 0.4233 - acc: 0.9237 - val_loss: 0.7309 - val_acc: 0.8468\n",
      "Epoch 33/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.9238Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:09 - loss: 0.6933 - acc: 0.8540\n",
      "Epoch 00033: val_acc did not improve from 0.89615\n",
      "781/781 [==============================] - 385s 493ms/step - loss: 0.4201 - acc: 0.9238 - val_loss: 0.6933 - val_acc: 0.8540\n",
      "Epoch 34/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4133 - acc: 0.9255Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.5150 - acc: 0.9019\n",
      "Epoch 00034: val_acc improved from 0.89615 to 0.90194, saving model to /content/drive/My Drive/saved-model-34-0.90.hdf5\n",
      "781/781 [==============================] - 387s 496ms/step - loss: 0.4133 - acc: 0.9255 - val_loss: 0.5150 - val_acc: 0.9019\n",
      "Epoch 35/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.9254Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:05 - loss: 0.5980 - acc: 0.8786\n",
      "Epoch 00035: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 381s 488ms/step - loss: 0.4131 - acc: 0.9254 - val_loss: 0.5980 - val_acc: 0.8786\n",
      "Epoch 36/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.9271Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:05 - loss: 0.5378 - acc: 0.8966\n",
      "Epoch 00036: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.4107 - acc: 0.9271 - val_loss: 0.5378 - val_acc: 0.8966\n",
      "Epoch 37/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.9279Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.6254 - acc: 0.8742\n",
      "Epoch 00037: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.4052 - acc: 0.9279 - val_loss: 0.6254 - val_acc: 0.8742\n",
      "Epoch 38/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.9280Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:08 - loss: 0.5857 - acc: 0.8772\n",
      "Epoch 00038: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 384s 491ms/step - loss: 0.4041 - acc: 0.9281 - val_loss: 0.5857 - val_acc: 0.8772\n",
      "Epoch 39/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.9291Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.6345 - acc: 0.8706\n",
      "Epoch 00039: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 381s 487ms/step - loss: 0.3987 - acc: 0.9291 - val_loss: 0.6345 - val_acc: 0.8706\n",
      "Epoch 40/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.9290Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.9289 - acc: 0.8071\n",
      "Epoch 00040: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.3988 - acc: 0.9290 - val_loss: 0.9289 - val_acc: 0.8071\n",
      "Epoch 41/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.9317Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.5332 - acc: 0.8995\n",
      "Epoch 00041: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 383s 490ms/step - loss: 0.3913 - acc: 0.9317 - val_loss: 0.5332 - val_acc: 0.8995\n",
      "Epoch 42/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.9306Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.5448 - acc: 0.8894\n",
      "Epoch 00042: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 383s 490ms/step - loss: 0.3929 - acc: 0.9306 - val_loss: 0.5448 - val_acc: 0.8894\n",
      "Epoch 43/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.9320Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.5904 - acc: 0.8768\n",
      "Epoch 00043: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 383s 491ms/step - loss: 0.3867 - acc: 0.9320 - val_loss: 0.5904 - val_acc: 0.8768\n",
      "Epoch 44/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.9332Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:05 - loss: 0.6217 - acc: 0.8718\n",
      "Epoch 00044: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.3862 - acc: 0.9333 - val_loss: 0.6217 - val_acc: 0.8718\n",
      "Epoch 45/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.9328Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.7410 - acc: 0.8408\n",
      "Epoch 00045: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 381s 488ms/step - loss: 0.3836 - acc: 0.9329 - val_loss: 0.7410 - val_acc: 0.8408\n",
      "Epoch 46/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.9337Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.5105 - acc: 0.9018\n",
      "Epoch 00046: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 383s 491ms/step - loss: 0.3831 - acc: 0.9337 - val_loss: 0.5105 - val_acc: 0.9018\n",
      "Epoch 47/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.9343Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.6704 - acc: 0.8608\n",
      "Epoch 00047: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 383s 490ms/step - loss: 0.3774 - acc: 0.9343 - val_loss: 0.6704 - val_acc: 0.8608\n",
      "Epoch 48/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.9351Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.5294 - acc: 0.8956\n",
      "Epoch 00048: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 381s 488ms/step - loss: 0.3761 - acc: 0.9351 - val_loss: 0.5294 - val_acc: 0.8956\n",
      "Epoch 49/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.9355Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.5247 - acc: 0.8934\n",
      "Epoch 00049: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.3732 - acc: 0.9355 - val_loss: 0.5247 - val_acc: 0.8934\n",
      "Epoch 50/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3747 - acc: 0.9352Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.6682 - acc: 0.8605\n",
      "Epoch 00050: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 381s 488ms/step - loss: 0.3747 - acc: 0.9352 - val_loss: 0.6682 - val_acc: 0.8605\n",
      "Epoch 51/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.9352Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.5337 - acc: 0.8924\n",
      "Epoch 00051: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 381s 488ms/step - loss: 0.3707 - acc: 0.9353 - val_loss: 0.5337 - val_acc: 0.8924\n",
      "Epoch 52/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.9354Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.6942 - acc: 0.8513\n",
      "Epoch 00052: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 383s 490ms/step - loss: 0.3705 - acc: 0.9354 - val_loss: 0.6942 - val_acc: 0.8513\n",
      "Epoch 53/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.9371Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.7732 - acc: 0.8538\n",
      "Epoch 00053: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.3651 - acc: 0.9370 - val_loss: 0.7732 - val_acc: 0.8538\n",
      "Epoch 54/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3666 - acc: 0.9369Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:05 - loss: 0.5760 - acc: 0.8836\n",
      "Epoch 00054: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.3665 - acc: 0.9369 - val_loss: 0.5760 - val_acc: 0.8836\n",
      "Epoch 55/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.9384Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.5742 - acc: 0.8842\n",
      "Epoch 00055: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 381s 488ms/step - loss: 0.3616 - acc: 0.9384 - val_loss: 0.5742 - val_acc: 0.8842\n",
      "Epoch 56/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.9376Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.7836 - acc: 0.8367\n",
      "Epoch 00056: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.3616 - acc: 0.9376 - val_loss: 0.7836 - val_acc: 0.8367\n",
      "Epoch 57/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.9381Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.5677 - acc: 0.8860\n",
      "Epoch 00057: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.3609 - acc: 0.9381 - val_loss: 0.5677 - val_acc: 0.8860\n",
      "Epoch 58/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.9396Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.7347 - acc: 0.8467\n",
      "Epoch 00058: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 490ms/step - loss: 0.3550 - acc: 0.9395 - val_loss: 0.7347 - val_acc: 0.8467\n",
      "Epoch 59/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3577 - acc: 0.9389Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.5272 - acc: 0.8945\n",
      "Epoch 00059: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 381s 488ms/step - loss: 0.3578 - acc: 0.9389 - val_loss: 0.5272 - val_acc: 0.8945\n",
      "Epoch 60/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.9380Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:05 - loss: 0.6305 - acc: 0.8652\n",
      "Epoch 00060: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 381s 488ms/step - loss: 0.3586 - acc: 0.9380 - val_loss: 0.6305 - val_acc: 0.8652\n",
      "Epoch 61/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.9389Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:06 - loss: 0.6511 - acc: 0.8758\n",
      "Epoch 00061: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.3545 - acc: 0.9388 - val_loss: 0.6511 - val_acc: 0.8758\n",
      "Epoch 62/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.9409Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.5452 - acc: 0.8856\n",
      "Epoch 00062: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 382s 489ms/step - loss: 0.3511 - acc: 0.9408 - val_loss: 0.5452 - val_acc: 0.8856\n",
      "Epoch 63/80\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.9404Epoch 1/80\n",
      "156/781 [====>.........................] - ETA: 1:07 - loss: 0.5599 - acc: 0.8868\n",
      "Epoch 00063: val_acc did not improve from 0.90194\n",
      "781/781 [==============================] - 380s 487ms/step - loss: 0.3500 - acc: 0.9404 - val_loss: 0.5599 - val_acc: 0.8868\n",
      "Epoch 64/80\n",
      "398/781 [==============>...............] - ETA: 2:59 - loss: 0.3491 - acc: 0.9398Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "history = model1.fit_generator(train_datagen.flow(X_train, y_train ,batch_size=128\n",
    " ),steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=80,\n",
    "                    validation_data=test_datagen.flow(X_test,y_test,batch_size=128),validation_steps = validation_steps,callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFdqEOLXQ8UQ"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "best_model = tf.keras.models.load_model('/content/drive/My Drive/saved-model-34-0.90.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LwG-Mxi5o9m"
   },
   "source": [
    "##### From epoch 34 , accuracy value of cv data = 0.9019. So test model is evalutated based on this model value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cMLn9KZzSzXo",
    "outputId": "2a28cda7-125b-4976-e98f-54ec317ab33a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 15s 95ms/step - loss: 0.5133 - acc: 0.9019\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test data\n",
    "score1 = best_model.evaluate_generator(test_datagen.flow(X_test, y_test, batch_size=64), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TZl90G7fS5zu",
    "outputId": "86de3a4f-32e2-4386-afd8-3a0a62de1952"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5132799552884072, 0.9019]"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "xJ93WrL5ANvm",
    "outputId": "28cffc45-8e04-4ddc-c760-f241b97d4d26"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmDLBCSpEHfAPPxVjjktvTzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as plt\n",
    "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
    " ax.plot(x, vy, 'b', label=\"Train Loss\")\n",
    " ax.plot(x, ty, 'r', label=\"Validation Loss\")\n",
    " plt.legend()\n",
    " plt.grid()\n",
    " fig.canvas.draw()\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "# list of epoch numbers\n",
    "x = list(range(1,64))\n",
    "vy = history.history['loss']\n",
    "ty = history.history['val_loss']\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "o33QWvDUBhgl",
    "outputId": "c426d558-ee77-4bb0-a7b8-fa9f4f314753"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAkVRFA9HhUBRFMXD+X9wnS3C/J3OuAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as plt\n",
    "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
    " ax.plot(x, vy, 'b', label=\"Train accuracy\")\n",
    " ax.plot(x, ty, 'r', label=\"Validation accuracy\")\n",
    " plt.legend()\n",
    " plt.grid()\n",
    " fig.canvas.draw()\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch') ; ax.set_ylabel('Accuracy')\n",
    "# list of epoch numbers\n",
    "x = list(range(1,64))\n",
    "vy = history.history['acc']\n",
    "ty = history.history['val_acc']\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vUiCfgaBTA7W",
    "outputId": "e1756a4f-0728-46c3-d37d-8f85b6a4f681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy [90.194]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Test Accuracy {}\". format(score1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrfMV8tw8qNm"
   },
   "source": [
    "#### Test Accuracy = 0.9019\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CNN on CIFR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
